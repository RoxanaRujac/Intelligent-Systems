{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6e1975d6-7528-4a5f-9f94-c0339e5d00c6",
      "metadata": {
        "id": "6e1975d6-7528-4a5f-9f94-c0339e5d00c6"
      },
      "source": [
        "## Lecture 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d386805-c408-4621-9872-105794a3153a",
      "metadata": {
        "id": "5d386805-c408-4621-9872-105794a3153a"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93042449-302f-4e1b-8315-b247a3d4d29d",
      "metadata": {
        "id": "93042449-302f-4e1b-8315-b247a3d4d29d"
      },
      "outputs": [],
      "source": [
        "x_data = [1.0, 2.0, 3.0]\n",
        "y_data = [2.0, 4.0, 6.0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e118399c-7ee0-4a39-bf21-3831d8f03d41",
      "metadata": {
        "id": "e118399c-7ee0-4a39-bf21-3831d8f03d41"
      },
      "outputs": [],
      "source": [
        "# our model for the forward pass\n",
        "def forward(x):\n",
        "    return x * w"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43226bfb-89e5-416d-b0d2-9eee6cfceb98",
      "metadata": {
        "id": "43226bfb-89e5-416d-b0d2-9eee6cfceb98"
      },
      "outputs": [],
      "source": [
        "# Loss function\n",
        "def loss(x, y):\n",
        "    y_pred = forward(x)\n",
        "    return (y_pred - y) * (y_pred - y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a7ad7a0-2c37-43c3-9c2f-a743fe047bb0",
      "metadata": {
        "id": "6a7ad7a0-2c37-43c3-9c2f-a743fe047bb0"
      },
      "outputs": [],
      "source": [
        "# List of weights/Mean square Error (Mse) for each input\n",
        "w_list = []\n",
        "mse_list = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "add94cef-d203-4aec-910f-bad2dac91ce9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "add94cef-d203-4aec-910f-bad2dac91ce9",
        "outputId": "2aa6e878-78ad-4cfd-fcde-783a47c466b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "w= 0.0\n",
            "\t 1.0 2.0 0.0 4.0\n",
            "\t 2.0 4.0 0.0 16.0\n",
            "\t 3.0 6.0 0.0 36.0\n",
            "MSE= 18.666666666666668\n",
            "w= 0.1\n",
            "\t 1.0 2.0 0.1 3.61\n",
            "\t 2.0 4.0 0.2 14.44\n",
            "\t 3.0 6.0 0.30000000000000004 32.49\n",
            "MSE= 16.846666666666668\n",
            "w= 0.2\n",
            "\t 1.0 2.0 0.2 3.24\n",
            "\t 2.0 4.0 0.4 12.96\n",
            "\t 3.0 6.0 0.6000000000000001 29.160000000000004\n",
            "MSE= 15.120000000000003\n",
            "w= 0.30000000000000004\n",
            "\t 1.0 2.0 0.30000000000000004 2.8899999999999997\n",
            "\t 2.0 4.0 0.6000000000000001 11.559999999999999\n",
            "\t 3.0 6.0 0.9000000000000001 26.009999999999998\n",
            "MSE= 13.486666666666665\n",
            "w= 0.4\n",
            "\t 1.0 2.0 0.4 2.5600000000000005\n",
            "\t 2.0 4.0 0.8 10.240000000000002\n",
            "\t 3.0 6.0 1.2000000000000002 23.04\n",
            "MSE= 11.946666666666667\n",
            "w= 0.5\n",
            "\t 1.0 2.0 0.5 2.25\n",
            "\t 2.0 4.0 1.0 9.0\n",
            "\t 3.0 6.0 1.5 20.25\n",
            "MSE= 10.5\n",
            "w= 0.6000000000000001\n",
            "\t 1.0 2.0 0.6000000000000001 1.9599999999999997\n",
            "\t 2.0 4.0 1.2000000000000002 7.839999999999999\n",
            "\t 3.0 6.0 1.8000000000000003 17.639999999999993\n",
            "MSE= 9.146666666666663\n",
            "w= 0.7000000000000001\n",
            "\t 1.0 2.0 0.7000000000000001 1.6899999999999995\n",
            "\t 2.0 4.0 1.4000000000000001 6.759999999999998\n",
            "\t 3.0 6.0 2.1 15.209999999999999\n",
            "MSE= 7.886666666666666\n",
            "w= 0.8\n",
            "\t 1.0 2.0 0.8 1.44\n",
            "\t 2.0 4.0 1.6 5.76\n",
            "\t 3.0 6.0 2.4000000000000004 12.959999999999997\n",
            "MSE= 6.719999999999999\n",
            "w= 0.9\n",
            "\t 1.0 2.0 0.9 1.2100000000000002\n",
            "\t 2.0 4.0 1.8 4.840000000000001\n",
            "\t 3.0 6.0 2.7 10.889999999999999\n",
            "MSE= 5.646666666666666\n",
            "w= 1.0\n",
            "\t 1.0 2.0 1.0 1.0\n",
            "\t 2.0 4.0 2.0 4.0\n",
            "\t 3.0 6.0 3.0 9.0\n",
            "MSE= 4.666666666666667\n",
            "w= 1.1\n",
            "\t 1.0 2.0 1.1 0.8099999999999998\n",
            "\t 2.0 4.0 2.2 3.2399999999999993\n",
            "\t 3.0 6.0 3.3000000000000003 7.289999999999998\n",
            "MSE= 3.779999999999999\n",
            "w= 1.2000000000000002\n",
            "\t 1.0 2.0 1.2000000000000002 0.6399999999999997\n",
            "\t 2.0 4.0 2.4000000000000004 2.5599999999999987\n",
            "\t 3.0 6.0 3.6000000000000005 5.759999999999997\n",
            "MSE= 2.986666666666665\n",
            "w= 1.3\n",
            "\t 1.0 2.0 1.3 0.48999999999999994\n",
            "\t 2.0 4.0 2.6 1.9599999999999997\n",
            "\t 3.0 6.0 3.9000000000000004 4.409999999999998\n",
            "MSE= 2.2866666666666657\n",
            "w= 1.4000000000000001\n",
            "\t 1.0 2.0 1.4000000000000001 0.3599999999999998\n",
            "\t 2.0 4.0 2.8000000000000003 1.4399999999999993\n",
            "\t 3.0 6.0 4.2 3.2399999999999993\n",
            "MSE= 1.6799999999999995\n",
            "w= 1.5\n",
            "\t 1.0 2.0 1.5 0.25\n",
            "\t 2.0 4.0 3.0 1.0\n",
            "\t 3.0 6.0 4.5 2.25\n",
            "MSE= 1.1666666666666667\n",
            "w= 1.6\n",
            "\t 1.0 2.0 1.6 0.15999999999999992\n",
            "\t 2.0 4.0 3.2 0.6399999999999997\n",
            "\t 3.0 6.0 4.800000000000001 1.4399999999999984\n",
            "MSE= 0.746666666666666\n",
            "w= 1.7000000000000002\n",
            "\t 1.0 2.0 1.7000000000000002 0.0899999999999999\n",
            "\t 2.0 4.0 3.4000000000000004 0.3599999999999996\n",
            "\t 3.0 6.0 5.1000000000000005 0.809999999999999\n",
            "MSE= 0.4199999999999995\n",
            "w= 1.8\n",
            "\t 1.0 2.0 1.8 0.03999999999999998\n",
            "\t 2.0 4.0 3.6 0.15999999999999992\n",
            "\t 3.0 6.0 5.4 0.3599999999999996\n",
            "MSE= 0.1866666666666665\n",
            "w= 1.9000000000000001\n",
            "\t 1.0 2.0 1.9000000000000001 0.009999999999999974\n",
            "\t 2.0 4.0 3.8000000000000003 0.0399999999999999\n",
            "\t 3.0 6.0 5.7 0.0899999999999999\n",
            "MSE= 0.046666666666666586\n",
            "w= 2.0\n",
            "\t 1.0 2.0 2.0 0.0\n",
            "\t 2.0 4.0 4.0 0.0\n",
            "\t 3.0 6.0 6.0 0.0\n",
            "MSE= 0.0\n",
            "w= 2.1\n",
            "\t 1.0 2.0 2.1 0.010000000000000018\n",
            "\t 2.0 4.0 4.2 0.04000000000000007\n",
            "\t 3.0 6.0 6.300000000000001 0.09000000000000043\n",
            "MSE= 0.046666666666666835\n",
            "w= 2.2\n",
            "\t 1.0 2.0 2.2 0.04000000000000007\n",
            "\t 2.0 4.0 4.4 0.16000000000000028\n",
            "\t 3.0 6.0 6.6000000000000005 0.36000000000000065\n",
            "MSE= 0.18666666666666698\n",
            "w= 2.3000000000000003\n",
            "\t 1.0 2.0 2.3000000000000003 0.09000000000000016\n",
            "\t 2.0 4.0 4.6000000000000005 0.36000000000000065\n",
            "\t 3.0 6.0 6.9 0.8100000000000006\n",
            "MSE= 0.42000000000000054\n",
            "w= 2.4000000000000004\n",
            "\t 1.0 2.0 2.4000000000000004 0.16000000000000028\n",
            "\t 2.0 4.0 4.800000000000001 0.6400000000000011\n",
            "\t 3.0 6.0 7.200000000000001 1.4400000000000026\n",
            "MSE= 0.7466666666666679\n",
            "w= 2.5\n",
            "\t 1.0 2.0 2.5 0.25\n",
            "\t 2.0 4.0 5.0 1.0\n",
            "\t 3.0 6.0 7.5 2.25\n",
            "MSE= 1.1666666666666667\n",
            "w= 2.6\n",
            "\t 1.0 2.0 2.6 0.3600000000000001\n",
            "\t 2.0 4.0 5.2 1.4400000000000004\n",
            "\t 3.0 6.0 7.800000000000001 3.2400000000000024\n",
            "MSE= 1.6800000000000008\n",
            "w= 2.7\n",
            "\t 1.0 2.0 2.7 0.49000000000000027\n",
            "\t 2.0 4.0 5.4 1.960000000000001\n",
            "\t 3.0 6.0 8.100000000000001 4.410000000000006\n",
            "MSE= 2.2866666666666693\n",
            "w= 2.8000000000000003\n",
            "\t 1.0 2.0 2.8000000000000003 0.6400000000000005\n",
            "\t 2.0 4.0 5.6000000000000005 2.560000000000002\n",
            "\t 3.0 6.0 8.4 5.760000000000002\n",
            "MSE= 2.986666666666668\n",
            "w= 2.9000000000000004\n",
            "\t 1.0 2.0 2.9000000000000004 0.8100000000000006\n",
            "\t 2.0 4.0 5.800000000000001 3.2400000000000024\n",
            "\t 3.0 6.0 8.700000000000001 7.290000000000005\n",
            "MSE= 3.780000000000003\n",
            "w= 3.0\n",
            "\t 1.0 2.0 3.0 1.0\n",
            "\t 2.0 4.0 6.0 4.0\n",
            "\t 3.0 6.0 9.0 9.0\n",
            "MSE= 4.666666666666667\n",
            "w= 3.1\n",
            "\t 1.0 2.0 3.1 1.2100000000000002\n",
            "\t 2.0 4.0 6.2 4.840000000000001\n",
            "\t 3.0 6.0 9.3 10.890000000000004\n",
            "MSE= 5.646666666666668\n",
            "w= 3.2\n",
            "\t 1.0 2.0 3.2 1.4400000000000004\n",
            "\t 2.0 4.0 6.4 5.760000000000002\n",
            "\t 3.0 6.0 9.600000000000001 12.96000000000001\n",
            "MSE= 6.720000000000003\n",
            "w= 3.3000000000000003\n",
            "\t 1.0 2.0 3.3000000000000003 1.6900000000000006\n",
            "\t 2.0 4.0 6.6000000000000005 6.7600000000000025\n",
            "\t 3.0 6.0 9.9 15.210000000000003\n",
            "MSE= 7.886666666666668\n",
            "w= 3.4000000000000004\n",
            "\t 1.0 2.0 3.4000000000000004 1.960000000000001\n",
            "\t 2.0 4.0 6.800000000000001 7.840000000000004\n",
            "\t 3.0 6.0 10.200000000000001 17.640000000000008\n",
            "MSE= 9.14666666666667\n",
            "w= 3.5\n",
            "\t 1.0 2.0 3.5 2.25\n",
            "\t 2.0 4.0 7.0 9.0\n",
            "\t 3.0 6.0 10.5 20.25\n",
            "MSE= 10.5\n",
            "w= 3.6\n",
            "\t 1.0 2.0 3.6 2.5600000000000005\n",
            "\t 2.0 4.0 7.2 10.240000000000002\n",
            "\t 3.0 6.0 10.8 23.040000000000006\n",
            "MSE= 11.94666666666667\n",
            "w= 3.7\n",
            "\t 1.0 2.0 3.7 2.8900000000000006\n",
            "\t 2.0 4.0 7.4 11.560000000000002\n",
            "\t 3.0 6.0 11.100000000000001 26.010000000000016\n",
            "MSE= 13.486666666666673\n",
            "w= 3.8000000000000003\n",
            "\t 1.0 2.0 3.8000000000000003 3.240000000000001\n",
            "\t 2.0 4.0 7.6000000000000005 12.960000000000004\n",
            "\t 3.0 6.0 11.4 29.160000000000004\n",
            "MSE= 15.120000000000005\n",
            "w= 3.9000000000000004\n",
            "\t 1.0 2.0 3.9000000000000004 3.610000000000001\n",
            "\t 2.0 4.0 7.800000000000001 14.440000000000005\n",
            "\t 3.0 6.0 11.700000000000001 32.49000000000001\n",
            "MSE= 16.84666666666667\n",
            "w= 4.0\n",
            "\t 1.0 2.0 4.0 4.0\n",
            "\t 2.0 4.0 8.0 16.0\n",
            "\t 3.0 6.0 12.0 36.0\n",
            "MSE= 18.666666666666668\n"
          ]
        }
      ],
      "source": [
        "for w in np.arange(0.0, 4.1, 0.1):\n",
        "    # Print the weights and initialize the lost\n",
        "    print(\"w=\", w)\n",
        "    l_sum = 0\n",
        "\n",
        "    for x_val, y_val in zip(x_data, y_data):\n",
        "        # For each input and output, calculate y_hat\n",
        "        # Compute the total loss and add to the total error\n",
        "        y_pred_val = forward(x_val)\n",
        "        l = loss(x_val, y_val)\n",
        "        l_sum += l\n",
        "        print(\"\\t\", x_val, y_val, y_pred_val, l)\n",
        "    # Now compute the Mean squared error (mse) of each\n",
        "    # Aggregate the weight/mse from this run\n",
        "    print(\"MSE=\", l_sum / len(x_data))\n",
        "    w_list.append(w)\n",
        "    mse_list.append(l_sum / len(x_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6bb533c6-3ce1-43b2-825f-10bbc7b9919e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "6bb533c6-3ce1-43b2-825f-10bbc7b9919e",
        "outputId": "97a70a65-4635-4353-8f1f-1e7c65a4685d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Plot it all\n",
        "plt.plot(w_list, mse_list)\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('w')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10bb5163-4c16-41a5-9e7b-c6964b634ac2",
      "metadata": {
        "id": "10bb5163-4c16-41a5-9e7b-c6964b634ac2"
      },
      "source": [
        "## Lecture 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8eeace1-d96e-426c-a9fb-eef01b1bbe14",
      "metadata": {
        "id": "d8eeace1-d96e-426c-a9fb-eef01b1bbe14"
      },
      "outputs": [],
      "source": [
        "w = 1.0  # a random guess: random value\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32a7c2d0-1eae-4a77-b96f-6b5c4c25e695",
      "metadata": {
        "id": "32a7c2d0-1eae-4a77-b96f-6b5c4c25e695"
      },
      "outputs": [],
      "source": [
        "# compute gradient\n",
        "def gradient(x, y):  # d_loss/d_w\n",
        "    return 2 * x * (x * w - y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22bbe594-2bd8-4465-8ddd-cd77d33c6c0c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22bbe594-2bd8-4465-8ddd-cd77d33c6c0c",
        "outputId": "5d8a129f-461e-4779-ab8f-35d39ca2d332"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction (before training) 4 4.0\n",
            "\tgrad:  1.0 2.0 -2.0\n",
            "\tgrad:  2.0 4.0 -7.84\n",
            "\tgrad:  3.0 6.0 -16.23\n",
            "progress: 0 w= 1.26 loss= 4.92\n",
            "\tgrad:  1.0 2.0 -1.48\n",
            "\tgrad:  2.0 4.0 -5.8\n",
            "\tgrad:  3.0 6.0 -12.0\n",
            "progress: 1 w= 1.45 loss= 2.69\n",
            "\tgrad:  1.0 2.0 -1.09\n",
            "\tgrad:  2.0 4.0 -4.29\n",
            "\tgrad:  3.0 6.0 -8.87\n",
            "progress: 2 w= 1.6 loss= 1.47\n",
            "\tgrad:  1.0 2.0 -0.81\n",
            "\tgrad:  2.0 4.0 -3.17\n",
            "\tgrad:  3.0 6.0 -6.56\n",
            "progress: 3 w= 1.7 loss= 0.8\n",
            "\tgrad:  1.0 2.0 -0.6\n",
            "\tgrad:  2.0 4.0 -2.34\n",
            "\tgrad:  3.0 6.0 -4.85\n",
            "progress: 4 w= 1.78 loss= 0.44\n",
            "\tgrad:  1.0 2.0 -0.44\n",
            "\tgrad:  2.0 4.0 -1.73\n",
            "\tgrad:  3.0 6.0 -3.58\n",
            "progress: 5 w= 1.84 loss= 0.24\n",
            "\tgrad:  1.0 2.0 -0.33\n",
            "\tgrad:  2.0 4.0 -1.28\n",
            "\tgrad:  3.0 6.0 -2.65\n",
            "progress: 6 w= 1.88 loss= 0.13\n",
            "\tgrad:  1.0 2.0 -0.24\n",
            "\tgrad:  2.0 4.0 -0.95\n",
            "\tgrad:  3.0 6.0 -1.96\n",
            "progress: 7 w= 1.91 loss= 0.07\n",
            "\tgrad:  1.0 2.0 -0.18\n",
            "\tgrad:  2.0 4.0 -0.7\n",
            "\tgrad:  3.0 6.0 -1.45\n",
            "progress: 8 w= 1.93 loss= 0.04\n",
            "\tgrad:  1.0 2.0 -0.13\n",
            "\tgrad:  2.0 4.0 -0.52\n",
            "\tgrad:  3.0 6.0 -1.07\n",
            "progress: 9 w= 1.95 loss= 0.02\n",
            "Predicted score (after training) 4 hours of studying:  7.804863933862125\n"
          ]
        }
      ],
      "source": [
        "# Before training\n",
        "print(\"Prediction (before training)\",  4, forward(4))\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(10):\n",
        "    for x_val, y_val in zip(x_data, y_data):\n",
        "        # Compute derivative w.r.t to the learned weights\n",
        "        # Update the weights\n",
        "        # Compute the loss and print progress\n",
        "        grad = gradient(x_val, y_val)\n",
        "        w = w - 0.01 * grad\n",
        "        print(\"\\tgrad: \", x_val, y_val, round(grad, 2))\n",
        "        l = loss(x_val, y_val)\n",
        "    print(\"progress:\", epoch, \"w=\", round(w, 2), \"loss=\", round(l, 2))\n",
        "\n",
        "# After training\n",
        "print(\"Predicted score (after training)\",  \"4 hours of studying: \", forward(4))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78d8c347-30f6-4658-89a9-35a0bd501af3",
      "metadata": {
        "id": "78d8c347-30f6-4658-89a9-35a0bd501af3"
      },
      "source": [
        "## Lecture 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff0be01c-5c33-4baa-adce-9b94e5eee2bb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ff0be01c-5c33-4baa-adce-9b94e5eee2bb",
        "outputId": "7af1f718-48d4-4e04-9df8-268b78a97732"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: torch==2.6.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.6.0+cu124)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.6.0->torchvision) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.6.0->torchvision) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m101.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m86.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install torchvision\n",
        "!pip install torch\n",
        "import torch\n",
        "import pdb\n",
        "\n",
        "w = torch.tensor([1.0], requires_grad=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "086a9449-18e7-449a-9954-9bd7167e0327",
      "metadata": {
        "id": "086a9449-18e7-449a-9954-9bd7167e0327"
      },
      "outputs": [],
      "source": [
        "def loss(y_pred, y_val):\n",
        "    return (y_pred - y_val) ** 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1259f5db-9491-4f8a-bdaf-37004b2f9530",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1259f5db-9491-4f8a-bdaf-37004b2f9530",
        "outputId": "37927ff9-90c8-45e2-ece0-33f6c1cbec08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction (before training) 4 4.0\n",
            "\tgrad:  1.0 2.0 -2.0\n",
            "\tgrad:  2.0 4.0 -7.840000152587891\n",
            "\tgrad:  3.0 6.0 -16.228801727294922\n",
            "Epoch: 0 | Loss: 7.315943717956543\n",
            "\tgrad:  1.0 2.0 -1.478623867034912\n",
            "\tgrad:  2.0 4.0 -5.796205520629883\n",
            "\tgrad:  3.0 6.0 -11.998146057128906\n",
            "Epoch: 1 | Loss: 3.9987640380859375\n",
            "\tgrad:  1.0 2.0 -1.0931644439697266\n",
            "\tgrad:  2.0 4.0 -4.285204887390137\n",
            "\tgrad:  3.0 6.0 -8.870372772216797\n",
            "Epoch: 2 | Loss: 2.1856532096862793\n",
            "\tgrad:  1.0 2.0 -0.8081896305084229\n",
            "\tgrad:  2.0 4.0 -3.1681032180786133\n",
            "\tgrad:  3.0 6.0 -6.557973861694336\n",
            "Epoch: 3 | Loss: 1.1946394443511963\n",
            "\tgrad:  1.0 2.0 -0.5975041389465332\n",
            "\tgrad:  2.0 4.0 -2.3422164916992188\n",
            "\tgrad:  3.0 6.0 -4.848389625549316\n",
            "Epoch: 4 | Loss: 0.6529689431190491\n",
            "\tgrad:  1.0 2.0 -0.4417421817779541\n",
            "\tgrad:  2.0 4.0 -1.7316293716430664\n",
            "\tgrad:  3.0 6.0 -3.58447265625\n",
            "Epoch: 5 | Loss: 0.35690122842788696\n",
            "\tgrad:  1.0 2.0 -0.3265852928161621\n",
            "\tgrad:  2.0 4.0 -1.2802143096923828\n",
            "\tgrad:  3.0 6.0 -2.650045394897461\n",
            "Epoch: 6 | Loss: 0.195076122879982\n",
            "\tgrad:  1.0 2.0 -0.24144840240478516\n",
            "\tgrad:  2.0 4.0 -0.9464778900146484\n",
            "\tgrad:  3.0 6.0 -1.9592113494873047\n",
            "Epoch: 7 | Loss: 0.10662525147199631\n",
            "\tgrad:  1.0 2.0 -0.17850565910339355\n",
            "\tgrad:  2.0 4.0 -0.699742317199707\n",
            "\tgrad:  3.0 6.0 -1.4484672546386719\n",
            "Epoch: 8 | Loss: 0.0582793727517128\n",
            "\tgrad:  1.0 2.0 -0.1319713592529297\n",
            "\tgrad:  2.0 4.0 -0.5173273086547852\n",
            "\tgrad:  3.0 6.0 -1.070866584777832\n",
            "Epoch: 9 | Loss: 0.03185431286692619\n",
            "Prediction (after training) 4 7.804864406585693\n"
          ]
        }
      ],
      "source": [
        "# Before training\n",
        "print(\"Prediction (before training)\",  4, forward(4).item())\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(10):\n",
        "    for x_val, y_val in zip(x_data, y_data):\n",
        "        y_pred = forward(x_val) # 1) Forward pass\n",
        "        l = loss(y_pred, y_val) # 2) Compute loss\n",
        "        l.backward() # 3) Back propagation to update weights\n",
        "        print(\"\\tgrad: \", x_val, y_val, w.grad.item())\n",
        "        w.data = w.data - 0.01 * w.grad.item()\n",
        "\n",
        "        # Manually zero the gradients after updating weights\n",
        "        w.grad.data.zero_()\n",
        "\n",
        "    print(f\"Epoch: {epoch} | Loss: {l.item()}\")\n",
        "\n",
        "# After training\n",
        "print(\"Prediction (after training)\",  4, forward(4).item())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a29b4ff-a26e-457d-a1cc-ef2f033bcec5",
      "metadata": {
        "id": "0a29b4ff-a26e-457d-a1cc-ef2f033bcec5"
      },
      "source": [
        "## Lecture 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2701f01a-7949-460a-944f-beb39b5fe35b",
      "metadata": {
        "id": "2701f01a-7949-460a-944f-beb39b5fe35b"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "import torch\n",
        "from torch import tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46de6a35-c88b-401e-a3dd-09d756eb6148",
      "metadata": {
        "id": "46de6a35-c88b-401e-a3dd-09d756eb6148"
      },
      "outputs": [],
      "source": [
        "x_data = tensor([[1.0], [2.0], [3.0]])\n",
        "y_data = tensor([[2.0], [4.0], [6.0]])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4d4103c-51a1-462a-86f2-e567bdf4bfbc",
      "metadata": {
        "id": "a4d4103c-51a1-462a-86f2-e567bdf4bfbc"
      },
      "outputs": [],
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        In the constructor we instantiate two nn.Linear module\n",
        "        \"\"\"\n",
        "        super(Model, self).__init__()\n",
        "        self.linear = torch.nn.Linear(1, 1)  # One in and one out\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        In the forward function we accept a Variable of input data and we must return\n",
        "        a Variable of output data. We can use Modules defined in the constructor as\n",
        "        well as arbitrary operators on Variables.\n",
        "        \"\"\"\n",
        "        y_pred = self.linear(x)\n",
        "        return y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eee66c7b-e8ec-4d0f-8b09-72053019f025",
      "metadata": {
        "id": "eee66c7b-e8ec-4d0f-8b09-72053019f025"
      },
      "outputs": [],
      "source": [
        "# our model\n",
        "model = Model()\n",
        "\n",
        "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
        "# in the SGD constructor will contain the learnable parameters of the two\n",
        "# nn.Linear modules which are members of the model.\n",
        "criterion = torch.nn.MSELoss(reduction='sum')\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13250f20-9db4-487c-8486-ec999109a28b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13250f20-9db4-487c-8486-ec999109a28b",
        "outputId": "8a532ce4-8190-400e-fe07-83c74b40eac0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 | Loss: 20.842309951782227 \n",
            "Epoch: 1 | Loss: 9.284193992614746 \n",
            "Epoch: 2 | Loss: 4.138767242431641 \n",
            "Epoch: 3 | Loss: 1.848087191581726 \n",
            "Epoch: 4 | Loss: 0.8282619714736938 \n",
            "Epoch: 5 | Loss: 0.3741834759712219 \n",
            "Epoch: 6 | Loss: 0.1719629168510437 \n",
            "Epoch: 7 | Loss: 0.08186215907335281 \n",
            "Epoch: 8 | Loss: 0.04167585074901581 \n",
            "Epoch: 9 | Loss: 0.023710716515779495 \n",
            "Epoch: 10 | Loss: 0.01563892513513565 \n",
            "Epoch: 11 | Loss: 0.01197260245680809 \n",
            "Epoch: 12 | Loss: 0.01026848889887333 \n",
            "Epoch: 13 | Loss: 0.009438889101147652 \n",
            "Epoch: 14 | Loss: 0.00899960845708847 \n",
            "Epoch: 15 | Loss: 0.008735117502510548 \n",
            "Epoch: 16 | Loss: 0.008549420163035393 \n",
            "Epoch: 17 | Loss: 0.008399724960327148 \n",
            "Epoch: 18 | Loss: 0.008267097175121307 \n",
            "Epoch: 19 | Loss: 0.008142996579408646 \n",
            "Epoch: 20 | Loss: 0.008023571223020554 \n",
            "Epoch: 21 | Loss: 0.00790722668170929 \n",
            "Epoch: 22 | Loss: 0.007793091703206301 \n",
            "Epoch: 23 | Loss: 0.007680927403271198 \n",
            "Epoch: 24 | Loss: 0.007570422720164061 \n",
            "Epoch: 25 | Loss: 0.007461570668965578 \n",
            "Epoch: 26 | Loss: 0.007354344241321087 \n",
            "Epoch: 27 | Loss: 0.007248624227941036 \n",
            "Epoch: 28 | Loss: 0.007144469767808914 \n",
            "Epoch: 29 | Loss: 0.007041772361844778 \n",
            "Epoch: 30 | Loss: 0.006940608378499746 \n",
            "Epoch: 31 | Loss: 0.006840845104306936 \n",
            "Epoch: 32 | Loss: 0.006742491386830807 \n",
            "Epoch: 33 | Loss: 0.006645626854151487 \n",
            "Epoch: 34 | Loss: 0.006550146266818047 \n",
            "Epoch: 35 | Loss: 0.0064559816382825375 \n",
            "Epoch: 36 | Loss: 0.00636317627504468 \n",
            "Epoch: 37 | Loss: 0.006271742284297943 \n",
            "Epoch: 38 | Loss: 0.006181605160236359 \n",
            "Epoch: 39 | Loss: 0.006092761643230915 \n",
            "Epoch: 40 | Loss: 0.006005173549056053 \n",
            "Epoch: 41 | Loss: 0.005918893963098526 \n",
            "Epoch: 42 | Loss: 0.005833826027810574 \n",
            "Epoch: 43 | Loss: 0.005749999545514584 \n",
            "Epoch: 44 | Loss: 0.005667370744049549 \n",
            "Epoch: 45 | Loss: 0.00558592239394784 \n",
            "Epoch: 46 | Loss: 0.005505645647644997 \n",
            "Epoch: 47 | Loss: 0.005426498595625162 \n",
            "Epoch: 48 | Loss: 0.005348545033484697 \n",
            "Epoch: 49 | Loss: 0.005271678790450096 \n",
            "Epoch: 50 | Loss: 0.005195893347263336 \n",
            "Epoch: 51 | Loss: 0.005121209658682346 \n",
            "Epoch: 52 | Loss: 0.00504763238132 \n",
            "Epoch: 53 | Loss: 0.004975072108209133 \n",
            "Epoch: 54 | Loss: 0.0049035819247365 \n",
            "Epoch: 55 | Loss: 0.0048331269063055515 \n",
            "Epoch: 56 | Loss: 0.0047636390663683414 \n",
            "Epoch: 57 | Loss: 0.0046952045522630215 \n",
            "Epoch: 58 | Loss: 0.004627696238458157 \n",
            "Epoch: 59 | Loss: 0.004561218433082104 \n",
            "Epoch: 60 | Loss: 0.004495665896683931 \n",
            "Epoch: 61 | Loss: 0.004431028850376606 \n",
            "Epoch: 62 | Loss: 0.004367364104837179 \n",
            "Epoch: 63 | Loss: 0.004304594360291958 \n",
            "Epoch: 64 | Loss: 0.004242729861289263 \n",
            "Epoch: 65 | Loss: 0.004181756637990475 \n",
            "Epoch: 66 | Loss: 0.004121676087379456 \n",
            "Epoch: 67 | Loss: 0.0040624202229082584 \n",
            "Epoch: 68 | Loss: 0.0040040635503828526 \n",
            "Epoch: 69 | Loss: 0.003946472890675068 \n",
            "Epoch: 70 | Loss: 0.0038897674530744553 \n",
            "Epoch: 71 | Loss: 0.003833875060081482 \n",
            "Epoch: 72 | Loss: 0.0037787947803735733 \n",
            "Epoch: 73 | Loss: 0.003724478418007493 \n",
            "Epoch: 74 | Loss: 0.003670950187370181 \n",
            "Epoch: 75 | Loss: 0.0036182005424052477 \n",
            "Epoch: 76 | Loss: 0.0035661906003952026 \n",
            "Epoch: 77 | Loss: 0.0035149191971868277 \n",
            "Epoch: 78 | Loss: 0.003464409150183201 \n",
            "Epoch: 79 | Loss: 0.0034146448597311974 \n",
            "Epoch: 80 | Loss: 0.0033655380830168724 \n",
            "Epoch: 81 | Loss: 0.00331718847155571 \n",
            "Epoch: 82 | Loss: 0.0032694945111870766 \n",
            "Epoch: 83 | Loss: 0.0032225323375314474 \n",
            "Epoch: 84 | Loss: 0.0031762076541781425 \n",
            "Epoch: 85 | Loss: 0.0031305993907153606 \n",
            "Epoch: 86 | Loss: 0.0030855918303132057 \n",
            "Epoch: 87 | Loss: 0.0030412280466407537 \n",
            "Epoch: 88 | Loss: 0.0029975385405123234 \n",
            "Epoch: 89 | Loss: 0.0029544434510171413 \n",
            "Epoch: 90 | Loss: 0.0029119960963726044 \n",
            "Epoch: 91 | Loss: 0.002870151773095131 \n",
            "Epoch: 92 | Loss: 0.0028288853354752064 \n",
            "Epoch: 93 | Loss: 0.0027882219292223454 \n",
            "Epoch: 94 | Loss: 0.0027481757570058107 \n",
            "Epoch: 95 | Loss: 0.0027086858171969652 \n",
            "Epoch: 96 | Loss: 0.002669721841812134 \n",
            "Epoch: 97 | Loss: 0.0026313592679798603 \n",
            "Epoch: 98 | Loss: 0.0025935648009181023 \n",
            "Epoch: 99 | Loss: 0.002556281629949808 \n",
            "Epoch: 100 | Loss: 0.0025195544585585594 \n",
            "Epoch: 101 | Loss: 0.0024833506904542446 \n",
            "Epoch: 102 | Loss: 0.0024476449470967054 \n",
            "Epoch: 103 | Loss: 0.00241247471421957 \n",
            "Epoch: 104 | Loss: 0.0023778085596859455 \n",
            "Epoch: 105 | Loss: 0.002343632746487856 \n",
            "Epoch: 106 | Loss: 0.0023099305108189583 \n",
            "Epoch: 107 | Loss: 0.0022767442278563976 \n",
            "Epoch: 108 | Loss: 0.0022440238390117884 \n",
            "Epoch: 109 | Loss: 0.0022117646876722574 \n",
            "Epoch: 110 | Loss: 0.0021799779497087 \n",
            "Epoch: 111 | Loss: 0.0021486717741936445 \n",
            "Epoch: 112 | Loss: 0.0021177688613533974 \n",
            "Epoch: 113 | Loss: 0.0020873420871794224 \n",
            "Epoch: 114 | Loss: 0.002057354198768735 \n",
            "Epoch: 115 | Loss: 0.002027766779065132 \n",
            "Epoch: 116 | Loss: 0.001998640364035964 \n",
            "Epoch: 117 | Loss: 0.0019698950927704573 \n",
            "Epoch: 118 | Loss: 0.0019415911519899964 \n",
            "Epoch: 119 | Loss: 0.0019136854680255055 \n",
            "Epoch: 120 | Loss: 0.0018862064462155104 \n",
            "Epoch: 121 | Loss: 0.0018590689869597554 \n",
            "Epoch: 122 | Loss: 0.0018323797266930342 \n",
            "Epoch: 123 | Loss: 0.001806024112738669 \n",
            "Epoch: 124 | Loss: 0.0017800875939428806 \n",
            "Epoch: 125 | Loss: 0.001754477503709495 \n",
            "Epoch: 126 | Loss: 0.0017292657867074013 \n",
            "Epoch: 127 | Loss: 0.001704423688352108 \n",
            "Epoch: 128 | Loss: 0.0016799401491880417 \n",
            "Epoch: 129 | Loss: 0.001655803993344307 \n",
            "Epoch: 130 | Loss: 0.0016319906571879983 \n",
            "Epoch: 131 | Loss: 0.001608536229468882 \n",
            "Epoch: 132 | Loss: 0.00158542743884027 \n",
            "Epoch: 133 | Loss: 0.0015626330859959126 \n",
            "Epoch: 134 | Loss: 0.001540160970762372 \n",
            "Epoch: 135 | Loss: 0.0015180445043370128 \n",
            "Epoch: 136 | Loss: 0.0014962159330025315 \n",
            "Epoch: 137 | Loss: 0.0014747332315891981 \n",
            "Epoch: 138 | Loss: 0.0014535142108798027 \n",
            "Epoch: 139 | Loss: 0.0014326341915875673 \n",
            "Epoch: 140 | Loss: 0.0014120578998699784 \n",
            "Epoch: 141 | Loss: 0.0013917498290538788 \n",
            "Epoch: 142 | Loss: 0.0013717396650463343 \n",
            "Epoch: 143 | Loss: 0.001352047431282699 \n",
            "Epoch: 144 | Loss: 0.0013326165499165654 \n",
            "Epoch: 145 | Loss: 0.0013134463224560022 \n",
            "Epoch: 146 | Loss: 0.0012945677153766155 \n",
            "Epoch: 147 | Loss: 0.0012759671080857515 \n",
            "Epoch: 148 | Loss: 0.0012576432200148702 \n",
            "Epoch: 149 | Loss: 0.0012395563535392284 \n",
            "Epoch: 150 | Loss: 0.0012217366602271795 \n",
            "Epoch: 151 | Loss: 0.0012042033486068249 \n",
            "Epoch: 152 | Loss: 0.001186896930448711 \n",
            "Epoch: 153 | Loss: 0.001169815193861723 \n",
            "Epoch: 154 | Loss: 0.001153009245172143 \n",
            "Epoch: 155 | Loss: 0.0011364563833922148 \n",
            "Epoch: 156 | Loss: 0.0011201126035302877 \n",
            "Epoch: 157 | Loss: 0.0011040031677111983 \n",
            "Epoch: 158 | Loss: 0.001088146585971117 \n",
            "Epoch: 159 | Loss: 0.0010725113097578287 \n",
            "Epoch: 160 | Loss: 0.0010570987360551953 \n",
            "Epoch: 161 | Loss: 0.0010419066529721022 \n",
            "Epoch: 162 | Loss: 0.001026941929012537 \n",
            "Epoch: 163 | Loss: 0.0010121557861566544 \n",
            "Epoch: 164 | Loss: 0.0009976217988878489 \n",
            "Epoch: 165 | Loss: 0.0009832751238718629 \n",
            "Epoch: 166 | Loss: 0.0009691427112556994 \n",
            "Epoch: 167 | Loss: 0.0009552347473800182 \n",
            "Epoch: 168 | Loss: 0.0009414951782673597 \n",
            "Epoch: 169 | Loss: 0.0009279737714678049 \n",
            "Epoch: 170 | Loss: 0.0009146416559815407 \n",
            "Epoch: 171 | Loss: 0.0009014949901029468 \n",
            "Epoch: 172 | Loss: 0.0008885232382453978 \n",
            "Epoch: 173 | Loss: 0.0008757589966990054 \n",
            "Epoch: 174 | Loss: 0.0008631640812382102 \n",
            "Epoch: 175 | Loss: 0.0008507637539878488 \n",
            "Epoch: 176 | Loss: 0.0008385407272726297 \n",
            "Epoch: 177 | Loss: 0.0008264813804998994 \n",
            "Epoch: 178 | Loss: 0.0008146188920363784 \n",
            "Epoch: 179 | Loss: 0.0008029047749005258 \n",
            "Epoch: 180 | Loss: 0.0007913589361123741 \n",
            "Epoch: 181 | Loss: 0.0007799973245710135 \n",
            "Epoch: 182 | Loss: 0.0007687903707846999 \n",
            "Epoch: 183 | Loss: 0.0007577334763482213 \n",
            "Epoch: 184 | Loss: 0.0007468424155376852 \n",
            "Epoch: 185 | Loss: 0.0007361090974882245 \n",
            "Epoch: 186 | Loss: 0.0007255394593812525 \n",
            "Epoch: 187 | Loss: 0.0007151008467189968 \n",
            "Epoch: 188 | Loss: 0.0007048319675959647 \n",
            "Epoch: 189 | Loss: 0.0006946971989236772 \n",
            "Epoch: 190 | Loss: 0.0006847011391073465 \n",
            "Epoch: 191 | Loss: 0.0006748787709511817 \n",
            "Epoch: 192 | Loss: 0.0006651722942478955 \n",
            "Epoch: 193 | Loss: 0.0006556121516041458 \n",
            "Epoch: 194 | Loss: 0.0006461891462095082 \n",
            "Epoch: 195 | Loss: 0.0006369033362716436 \n",
            "Epoch: 196 | Loss: 0.0006277459324337542 \n",
            "Epoch: 197 | Loss: 0.0006187198450788856 \n",
            "Epoch: 198 | Loss: 0.0006098281010054052 \n",
            "Epoch: 199 | Loss: 0.0006010705255903304 \n",
            "Epoch: 200 | Loss: 0.0005924443248659372 \n",
            "Epoch: 201 | Loss: 0.0005839169025421143 \n",
            "Epoch: 202 | Loss: 0.0005755280144512653 \n",
            "Epoch: 203 | Loss: 0.0005672579864040017 \n",
            "Epoch: 204 | Loss: 0.0005591015215031803 \n",
            "Epoch: 205 | Loss: 0.0005510749761015177 \n",
            "Epoch: 206 | Loss: 0.0005431512836366892 \n",
            "Epoch: 207 | Loss: 0.0005353400483727455 \n",
            "Epoch: 208 | Loss: 0.0005276468582451344 \n",
            "Epoch: 209 | Loss: 0.0005200717132538557 \n",
            "Epoch: 210 | Loss: 0.0005126051837578416 \n",
            "Epoch: 211 | Loss: 0.0005052267806604505 \n",
            "Epoch: 212 | Loss: 0.0004979679360985756 \n",
            "Epoch: 213 | Loss: 0.0004908093251287937 \n",
            "Epoch: 214 | Loss: 0.0004837472806684673 \n",
            "Epoch: 215 | Loss: 0.0004767960635945201 \n",
            "Epoch: 216 | Loss: 0.00046995445154607296 \n",
            "Epoch: 217 | Loss: 0.00046318728709593415 \n",
            "Epoch: 218 | Loss: 0.00045654369750991464 \n",
            "Epoch: 219 | Loss: 0.00044998512021265924 \n",
            "Epoch: 220 | Loss: 0.00044350954703986645 \n",
            "Epoch: 221 | Loss: 0.0004371380200609565 \n",
            "Epoch: 222 | Loss: 0.00043085598736070096 \n",
            "Epoch: 223 | Loss: 0.0004246626340318471 \n",
            "Epoch: 224 | Loss: 0.0004185604630038142 \n",
            "Epoch: 225 | Loss: 0.0004125367267988622 \n",
            "Epoch: 226 | Loss: 0.00040661057573743165 \n",
            "Epoch: 227 | Loss: 0.00040077033918350935 \n",
            "Epoch: 228 | Loss: 0.0003950174432247877 \n",
            "Epoch: 229 | Loss: 0.000389338267268613 \n",
            "Epoch: 230 | Loss: 0.00038374235737137496 \n",
            "Epoch: 231 | Loss: 0.00037822200101800263 \n",
            "Epoch: 232 | Loss: 0.0003727910225279629 \n",
            "Epoch: 233 | Loss: 0.00036743166856467724 \n",
            "Epoch: 234 | Loss: 0.00036214542342349887 \n",
            "Epoch: 235 | Loss: 0.00035695458063855767 \n",
            "Epoch: 236 | Loss: 0.00035180995473638177 \n",
            "Epoch: 237 | Loss: 0.00034676759969443083 \n",
            "Epoch: 238 | Loss: 0.00034177026827819645 \n",
            "Epoch: 239 | Loss: 0.00033686470123939216 \n",
            "Epoch: 240 | Loss: 0.00033201789483428 \n",
            "Epoch: 241 | Loss: 0.0003272514441050589 \n",
            "Epoch: 242 | Loss: 0.0003225455293431878 \n",
            "Epoch: 243 | Loss: 0.000317910045851022 \n",
            "Epoch: 244 | Loss: 0.0003133420250378549 \n",
            "Epoch: 245 | Loss: 0.00030884388252161443 \n",
            "Epoch: 246 | Loss: 0.000304399523884058 \n",
            "Epoch: 247 | Loss: 0.0003000267024617642 \n",
            "Epoch: 248 | Loss: 0.00029572047060355544 \n",
            "Epoch: 249 | Loss: 0.0002914632495958358 \n",
            "Epoch: 250 | Loss: 0.0002872782642953098 \n",
            "Epoch: 251 | Loss: 0.00028314217342995107 \n",
            "Epoch: 252 | Loss: 0.00027908367337659 \n",
            "Epoch: 253 | Loss: 0.0002750630956143141 \n",
            "Epoch: 254 | Loss: 0.0002711138513404876 \n",
            "Epoch: 255 | Loss: 0.0002672131231520325 \n",
            "Epoch: 256 | Loss: 0.00026337840245105326 \n",
            "Epoch: 257 | Loss: 0.0002595845144242048 \n",
            "Epoch: 258 | Loss: 0.0002558665000833571 \n",
            "Epoch: 259 | Loss: 0.00025218731025233865 \n",
            "Epoch: 260 | Loss: 0.0002485595177859068 \n",
            "Epoch: 261 | Loss: 0.00024498169659636915 \n",
            "Epoch: 262 | Loss: 0.00024146592477336526 \n",
            "Epoch: 263 | Loss: 0.0002380015648668632 \n",
            "Epoch: 264 | Loss: 0.00023457471979781985 \n",
            "Epoch: 265 | Loss: 0.0002312072174390778 \n",
            "Epoch: 266 | Loss: 0.00022788773640058935 \n",
            "Epoch: 267 | Loss: 0.00022460884065367281 \n",
            "Epoch: 268 | Loss: 0.00022137619089335203 \n",
            "Epoch: 269 | Loss: 0.00021820131223648787 \n",
            "Epoch: 270 | Loss: 0.0002150613145204261 \n",
            "Epoch: 271 | Loss: 0.00021196862508077174 \n",
            "Epoch: 272 | Loss: 0.00020892437896691263 \n",
            "Epoch: 273 | Loss: 0.00020592135842889547 \n",
            "Epoch: 274 | Loss: 0.0002029632596531883 \n",
            "Epoch: 275 | Loss: 0.00020004711404908448 \n",
            "Epoch: 276 | Loss: 0.00019716759561561048 \n",
            "Epoch: 277 | Loss: 0.0001943365205079317 \n",
            "Epoch: 278 | Loss: 0.00019154761685058475 \n",
            "Epoch: 279 | Loss: 0.00018879005801863968 \n",
            "Epoch: 280 | Loss: 0.00018607950187288225 \n",
            "Epoch: 281 | Loss: 0.00018340424867346883 \n",
            "Epoch: 282 | Loss: 0.00018077103595715016 \n",
            "Epoch: 283 | Loss: 0.00017816931358538568 \n",
            "Epoch: 284 | Loss: 0.00017560338892508298 \n",
            "Epoch: 285 | Loss: 0.00017309047689195722 \n",
            "Epoch: 286 | Loss: 0.0001705941976979375 \n",
            "Epoch: 287 | Loss: 0.0001681461581028998 \n",
            "Epoch: 288 | Loss: 0.00016572632011957467 \n",
            "Epoch: 289 | Loss: 0.00016335373220499605 \n",
            "Epoch: 290 | Loss: 0.00016100048378575593 \n",
            "Epoch: 291 | Loss: 0.00015868261107243598 \n",
            "Epoch: 292 | Loss: 0.0001564084377605468 \n",
            "Epoch: 293 | Loss: 0.00015416296082548797 \n",
            "Epoch: 294 | Loss: 0.00015194161096587777 \n",
            "Epoch: 295 | Loss: 0.00014975914382375777 \n",
            "Epoch: 296 | Loss: 0.00014760800695512444 \n",
            "Epoch: 297 | Loss: 0.0001454843586543575 \n",
            "Epoch: 298 | Loss: 0.00014339489280246198 \n",
            "Epoch: 299 | Loss: 0.00014133712102193385 \n",
            "Epoch: 300 | Loss: 0.00013930384011473507 \n",
            "Epoch: 301 | Loss: 0.00013730025966651738 \n",
            "Epoch: 302 | Loss: 0.00013532745651900768 \n",
            "Epoch: 303 | Loss: 0.0001333836989942938 \n",
            "Epoch: 304 | Loss: 0.0001314639812335372 \n",
            "Epoch: 305 | Loss: 0.00012957738363184035 \n",
            "Epoch: 306 | Loss: 0.0001277136179851368 \n",
            "Epoch: 307 | Loss: 0.0001258790143765509 \n",
            "Epoch: 308 | Loss: 0.00012406740279402584 \n",
            "Epoch: 309 | Loss: 0.00012228306150063872 \n",
            "Epoch: 310 | Loss: 0.00012052568490616977 \n",
            "Epoch: 311 | Loss: 0.00011879180965479463 \n",
            "Epoch: 312 | Loss: 0.00011709186946973205 \n",
            "Epoch: 313 | Loss: 0.00011541169078554958 \n",
            "Epoch: 314 | Loss: 0.0001137510989792645 \n",
            "Epoch: 315 | Loss: 0.00011211240052944049 \n",
            "Epoch: 316 | Loss: 0.00011050268221879378 \n",
            "Epoch: 317 | Loss: 0.0001089143188437447 \n",
            "Epoch: 318 | Loss: 0.00010735131218098104 \n",
            "Epoch: 319 | Loss: 0.00010580439993645996 \n",
            "Epoch: 320 | Loss: 0.00010428531822981313 \n",
            "Epoch: 321 | Loss: 0.00010279138223268092 \n",
            "Epoch: 322 | Loss: 0.00010131523595191538 \n",
            "Epoch: 323 | Loss: 9.985674842027947e-05 \n",
            "Epoch: 324 | Loss: 9.841979044722393e-05 \n",
            "Epoch: 325 | Loss: 9.700641385279596e-05 \n",
            "Epoch: 326 | Loss: 9.561236220179126e-05 \n",
            "Epoch: 327 | Loss: 9.423631126992404e-05 \n",
            "Epoch: 328 | Loss: 9.287863213103265e-05 \n",
            "Epoch: 329 | Loss: 9.154470899375156e-05 \n",
            "Epoch: 330 | Loss: 9.023092570714653e-05 \n",
            "Epoch: 331 | Loss: 8.893596532288939e-05 \n",
            "Epoch: 332 | Loss: 8.765689563006163e-05 \n",
            "Epoch: 333 | Loss: 8.639627776574343e-05 \n",
            "Epoch: 334 | Loss: 8.515390800312161e-05 \n",
            "Epoch: 335 | Loss: 8.393066673306748e-05 \n",
            "Epoch: 336 | Loss: 8.272578998003155e-05 \n",
            "Epoch: 337 | Loss: 8.153539238264784e-05 \n",
            "Epoch: 338 | Loss: 8.036771032493562e-05 \n",
            "Epoch: 339 | Loss: 7.920688949525356e-05 \n",
            "Epoch: 340 | Loss: 7.80699192546308e-05 \n",
            "Epoch: 341 | Loss: 7.69498510635458e-05 \n",
            "Epoch: 342 | Loss: 7.583988917758688e-05 \n",
            "Epoch: 343 | Loss: 7.475458551198244e-05 \n",
            "Epoch: 344 | Loss: 7.36761066946201e-05 \n",
            "Epoch: 345 | Loss: 7.261685823323205e-05 \n",
            "Epoch: 346 | Loss: 7.157758227549493e-05 \n",
            "Epoch: 347 | Loss: 7.054725574562326e-05 \n",
            "Epoch: 348 | Loss: 6.953312549740076e-05 \n",
            "Epoch: 349 | Loss: 6.853308877907693e-05 \n",
            "Epoch: 350 | Loss: 6.754744390491396e-05 \n",
            "Epoch: 351 | Loss: 6.657985795754939e-05 \n",
            "Epoch: 352 | Loss: 6.562302587553859e-05 \n",
            "Epoch: 353 | Loss: 6.467873026849702e-05 \n",
            "Epoch: 354 | Loss: 6.374962686095387e-05 \n",
            "Epoch: 355 | Loss: 6.283370748860762e-05 \n",
            "Epoch: 356 | Loss: 6.193127046572044e-05 \n",
            "Epoch: 357 | Loss: 6.103854684624821e-05 \n",
            "Epoch: 358 | Loss: 6.0162219597259536e-05 \n",
            "Epoch: 359 | Loss: 5.929626422584988e-05 \n",
            "Epoch: 360 | Loss: 5.8445471950108185e-05 \n",
            "Epoch: 361 | Loss: 5.7604804169386625e-05 \n",
            "Epoch: 362 | Loss: 5.6780299928504974e-05 \n",
            "Epoch: 363 | Loss: 5.5964348575798795e-05 \n",
            "Epoch: 364 | Loss: 5.515862721949816e-05 \n",
            "Epoch: 365 | Loss: 5.436388892121613e-05 \n",
            "Epoch: 366 | Loss: 5.358257840271108e-05 \n",
            "Epoch: 367 | Loss: 5.281157064018771e-05 \n",
            "Epoch: 368 | Loss: 5.2056635468034074e-05 \n",
            "Epoch: 369 | Loss: 5.130380668560974e-05 \n",
            "Epoch: 370 | Loss: 5.0568862207001075e-05 \n",
            "Epoch: 371 | Loss: 4.984291808796115e-05 \n",
            "Epoch: 372 | Loss: 4.912751683150418e-05 \n",
            "Epoch: 373 | Loss: 4.8420526582049206e-05 \n",
            "Epoch: 374 | Loss: 4.772508691530675e-05 \n",
            "Epoch: 375 | Loss: 4.7036272007972e-05 \n",
            "Epoch: 376 | Loss: 4.6362358261831105e-05 \n",
            "Epoch: 377 | Loss: 4.569606244331226e-05 \n",
            "Epoch: 378 | Loss: 4.503731906879693e-05 \n",
            "Epoch: 379 | Loss: 4.439304393599741e-05 \n",
            "Epoch: 380 | Loss: 4.375494972919114e-05 \n",
            "Epoch: 381 | Loss: 4.312529199523851e-05 \n",
            "Epoch: 382 | Loss: 4.250550773576833e-05 \n",
            "Epoch: 383 | Loss: 4.189622632111423e-05 \n",
            "Epoch: 384 | Loss: 4.129358785576187e-05 \n",
            "Epoch: 385 | Loss: 4.070162322022952e-05 \n",
            "Epoch: 386 | Loss: 4.0115770389093086e-05 \n",
            "Epoch: 387 | Loss: 3.953672421630472e-05 \n",
            "Epoch: 388 | Loss: 3.8970960304141045e-05 \n",
            "Epoch: 389 | Loss: 3.841179932351224e-05 \n",
            "Epoch: 390 | Loss: 3.78584663849324e-05 \n",
            "Epoch: 391 | Loss: 3.7314828659873456e-05 \n",
            "Epoch: 392 | Loss: 3.677618951769546e-05 \n",
            "Epoch: 393 | Loss: 3.624845703598112e-05 \n",
            "Epoch: 394 | Loss: 3.572662899387069e-05 \n",
            "Epoch: 395 | Loss: 3.521409962559119e-05 \n",
            "Epoch: 396 | Loss: 3.470836236374453e-05 \n",
            "Epoch: 397 | Loss: 3.421035944484174e-05 \n",
            "Epoch: 398 | Loss: 3.37166347890161e-05 \n",
            "Epoch: 399 | Loss: 3.323185592307709e-05 \n",
            "Epoch: 400 | Loss: 3.275757990195416e-05 \n",
            "Epoch: 401 | Loss: 3.228671266697347e-05 \n",
            "Epoch: 402 | Loss: 3.181892680004239e-05 \n",
            "Epoch: 403 | Loss: 3.136269515380263e-05 \n",
            "Epoch: 404 | Loss: 3.091137114097364e-05 \n",
            "Epoch: 405 | Loss: 3.0468776458292268e-05 \n",
            "Epoch: 406 | Loss: 3.0031918868189678e-05 \n",
            "Epoch: 407 | Loss: 2.9596954846056178e-05 \n",
            "Epoch: 408 | Loss: 2.917144593084231e-05 \n",
            "Epoch: 409 | Loss: 2.8752128855558112e-05 \n",
            "Epoch: 410 | Loss: 2.8342041332507506e-05 \n",
            "Epoch: 411 | Loss: 2.793367275444325e-05 \n",
            "Epoch: 412 | Loss: 2.7530706574907526e-05 \n",
            "Epoch: 413 | Loss: 2.713369576667901e-05 \n",
            "Epoch: 414 | Loss: 2.6745281502371654e-05 \n",
            "Epoch: 415 | Loss: 2.6362949938629754e-05 \n",
            "Epoch: 416 | Loss: 2.598515129648149e-05 \n",
            "Epoch: 417 | Loss: 2.5611549062887207e-05 \n",
            "Epoch: 418 | Loss: 2.524182855268009e-05 \n",
            "Epoch: 419 | Loss: 2.4878558178897947e-05 \n",
            "Epoch: 420 | Loss: 2.451993896102067e-05 \n",
            "Epoch: 421 | Loss: 2.4168208256014623e-05 \n",
            "Epoch: 422 | Loss: 2.3821005015634e-05 \n",
            "Epoch: 423 | Loss: 2.347913323319517e-05 \n",
            "Epoch: 424 | Loss: 2.314112862222828e-05 \n",
            "Epoch: 425 | Loss: 2.2810298105468974e-05 \n",
            "Epoch: 426 | Loss: 2.2481573978438973e-05 \n",
            "Epoch: 427 | Loss: 2.215660242654849e-05 \n",
            "Epoch: 428 | Loss: 2.1840789486304857e-05 \n",
            "Epoch: 429 | Loss: 2.1525898773688823e-05 \n",
            "Epoch: 430 | Loss: 2.1216239474597387e-05 \n",
            "Epoch: 431 | Loss: 2.0910947569063865e-05 \n",
            "Epoch: 432 | Loss: 2.0612089429050684e-05 \n",
            "Epoch: 433 | Loss: 2.0315648725954816e-05 \n",
            "Epoch: 434 | Loss: 2.0023178876726888e-05 \n",
            "Epoch: 435 | Loss: 1.9736180547624826e-05 \n",
            "Epoch: 436 | Loss: 1.945279655046761e-05 \n",
            "Epoch: 437 | Loss: 1.9172228348907083e-05 \n",
            "Epoch: 438 | Loss: 1.889748455141671e-05 \n",
            "Epoch: 439 | Loss: 1.862472709035501e-05 \n",
            "Epoch: 440 | Loss: 1.8357437511440367e-05 \n",
            "Epoch: 441 | Loss: 1.8092083337251097e-05 \n",
            "Epoch: 442 | Loss: 1.7834798200055957e-05 \n",
            "Epoch: 443 | Loss: 1.757667268975638e-05 \n",
            "Epoch: 444 | Loss: 1.7324298823950812e-05 \n",
            "Epoch: 445 | Loss: 1.7075915820896626e-05 \n",
            "Epoch: 446 | Loss: 1.6829326341394335e-05 \n",
            "Epoch: 447 | Loss: 1.6587369827902876e-05 \n",
            "Epoch: 448 | Loss: 1.6351397789549083e-05 \n",
            "Epoch: 449 | Loss: 1.6114547179313377e-05 \n",
            "Epoch: 450 | Loss: 1.5885219909250736e-05 \n",
            "Epoch: 451 | Loss: 1.5655696188332513e-05 \n",
            "Epoch: 452 | Loss: 1.54301269503776e-05 \n",
            "Epoch: 453 | Loss: 1.5207327123789582e-05 \n",
            "Epoch: 454 | Loss: 1.4989522242103703e-05 \n",
            "Epoch: 455 | Loss: 1.4774853298149537e-05 \n",
            "Epoch: 456 | Loss: 1.4561513125954662e-05 \n",
            "Epoch: 457 | Loss: 1.4351928257383406e-05 \n",
            "Epoch: 458 | Loss: 1.4146701687423047e-05 \n",
            "Epoch: 459 | Loss: 1.394338960380992e-05 \n",
            "Epoch: 460 | Loss: 1.3742625014856458e-05 \n",
            "Epoch: 461 | Loss: 1.3545244655688293e-05 \n",
            "Epoch: 462 | Loss: 1.3351201232580934e-05 \n",
            "Epoch: 463 | Loss: 1.3159825357433874e-05 \n",
            "Epoch: 464 | Loss: 1.2969831004738808e-05 \n",
            "Epoch: 465 | Loss: 1.2782674275513273e-05 \n",
            "Epoch: 466 | Loss: 1.259997134184232e-05 \n",
            "Epoch: 467 | Loss: 1.2418584447004832e-05 \n",
            "Epoch: 468 | Loss: 1.2239936040714383e-05 \n",
            "Epoch: 469 | Loss: 1.206561046274146e-05 \n",
            "Epoch: 470 | Loss: 1.1891534995811526e-05 \n",
            "Epoch: 471 | Loss: 1.1719520443875808e-05 \n",
            "Epoch: 472 | Loss: 1.1550736417120788e-05 \n",
            "Epoch: 473 | Loss: 1.1384744539100211e-05 \n",
            "Epoch: 474 | Loss: 1.1220731721550692e-05 \n",
            "Epoch: 475 | Loss: 1.1059844837291166e-05 \n",
            "Epoch: 476 | Loss: 1.0901653695327695e-05 \n",
            "Epoch: 477 | Loss: 1.0743078746600077e-05 \n",
            "Epoch: 478 | Loss: 1.0591339560050983e-05 \n",
            "Epoch: 479 | Loss: 1.0439178367960267e-05 \n",
            "Epoch: 480 | Loss: 1.0288117664458696e-05 \n",
            "Epoch: 481 | Loss: 1.01394552984857e-05 \n",
            "Epoch: 482 | Loss: 9.994629181164782e-06 \n",
            "Epoch: 483 | Loss: 9.850846254266798e-06 \n",
            "Epoch: 484 | Loss: 9.709372534416616e-06 \n",
            "Epoch: 485 | Loss: 9.570722795615438e-06 \n",
            "Epoch: 486 | Loss: 9.431106263946276e-06 \n",
            "Epoch: 487 | Loss: 9.297123142459895e-06 \n",
            "Epoch: 488 | Loss: 9.162162314169109e-06 \n",
            "Epoch: 489 | Loss: 9.031333320308477e-06 \n",
            "Epoch: 490 | Loss: 8.900059583538678e-06 \n",
            "Epoch: 491 | Loss: 8.773186891630758e-06 \n",
            "Epoch: 492 | Loss: 8.647739377920516e-06 \n",
            "Epoch: 493 | Loss: 8.521669769834261e-06 \n",
            "Epoch: 494 | Loss: 8.401240847888403e-06 \n",
            "Epoch: 495 | Loss: 8.279998837679159e-06 \n",
            "Epoch: 496 | Loss: 8.161629921232816e-06 \n",
            "Epoch: 497 | Loss: 8.044113201322034e-06 \n",
            "Epoch: 498 | Loss: 7.927286787889898e-06 \n",
            "Epoch: 499 | Loss: 7.81391190685099e-06 \n"
          ]
        }
      ],
      "source": [
        "# Training loop\n",
        "for epoch in range(500):\n",
        "    # 1) Forward pass: Compute predicted y by passing x to the model\n",
        "    y_pred = model(x_data)\n",
        "\n",
        "    # 2) Compute and print loss\n",
        "    loss = criterion(y_pred, y_data)\n",
        "    print(f'Epoch: {epoch} | Loss: {loss.item()} ')\n",
        "\n",
        "    # Zero gradients, perform a backward pass, and update the weights.\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6e97c3c-a025-4149-bf36-9086ad25ae1e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6e97c3c-a025-4149-bf36-9086ad25ae1e",
        "outputId": "58df1b2f-7953-419f-f0d8-11526f457df3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction (after training) 4 7.996786594390869\n"
          ]
        }
      ],
      "source": [
        "# After training\n",
        "hour_var = tensor([[4.0]])\n",
        "y_pred = model(hour_var)\n",
        "print(\"Prediction (after training)\",  4, model(hour_var).data[0][0].item())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4af07d96-ef2c-47c5-ae40-0a1056e6a54d",
      "metadata": {
        "id": "4af07d96-ef2c-47c5-ae40-0a1056e6a54d"
      },
      "source": [
        "## Lecture 6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a149e1b9-428f-4e66-9a7d-5f0800f0d45b",
      "metadata": {
        "id": "a149e1b9-428f-4e66-9a7d-5f0800f0d45b"
      },
      "outputs": [],
      "source": [
        "# Training data and ground truth\n",
        "x_data = tensor([[1.0], [2.0], [3.0], [4.0]])\n",
        "y_data = tensor([[0.], [0.], [1.], [1.]])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd8395a7-8ce2-4f10-8651-2e1311ae92ad",
      "metadata": {
        "id": "dd8395a7-8ce2-4f10-8651-2e1311ae92ad"
      },
      "outputs": [],
      "source": [
        "from torch import sigmoid\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
        "# in the SGD constructor will contain the learnable parameters of the two\n",
        "# nn.Linear modules which are members of the model.\n",
        "criterion = nn.BCELoss(reduction='mean')\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4568beb-a8af-46b5-b3dd-eaedd3539b54",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4568beb-a8af-46b5-b3dd-eaedd3539b54",
        "outputId": "f9011df9-68ff-44f1-d332-b1791f3c9947"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Let's predict the hours need to score above 50%\n",
            "==================================================\n",
            "Prediction after 1 hour of training: 2.0024 | Above 50%: True\n",
            "Prediction after 7 hours of training: 13.9912 | Above 50%: True\n"
          ]
        }
      ],
      "source": [
        "# After training\n",
        "print(f'\\nLet\\'s predict the hours need to score above 50%\\n{\"=\" * 50}')\n",
        "hour_var = model(tensor([[1.0]]))\n",
        "print(f'Prediction after 1 hour of training: {hour_var.item():.4f} | Above 50%: {hour_var.item() > 0.5}')\n",
        "hour_var = model(tensor([[7.0]]))\n",
        "print(f'Prediction after 7 hours of training: {hour_var.item():.4f} | Above 50%: { hour_var.item() > 0.5}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc2e1907-df9f-4c51-9e43-2db2f03c009f",
      "metadata": {
        "id": "bc2e1907-df9f-4c51-9e43-2db2f03c009f"
      },
      "source": [
        "## Lecture 7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5fdda0c0-7b21-493b-866b-d2a5965ed6e0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fdda0c0-7b21-493b-866b-d2a5965ed6e0",
        "outputId": "ca03ccef-a80c-4a7c-8d1c-af4b822262ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X's shape: torch.Size([768, 8]) | Y's shape: torch.Size([768, 1])\n"
          ]
        }
      ],
      "source": [
        "from torch import from_numpy, tensor\n",
        "import numpy as np\n",
        "\n",
        "xy = np.loadtxt('./diabetes.csv', delimiter=',', dtype=np.float32, skiprows=1)\n",
        "x_data = from_numpy(xy[:, 0:-1])\n",
        "y_data = from_numpy(xy[:, [-1]])\n",
        "print(f'X\\'s shape: {x_data.shape} | Y\\'s shape: {y_data.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "729a7b55-ed4f-4167-a190-cf990cb82236",
      "metadata": {
        "id": "729a7b55-ed4f-4167-a190-cf990cb82236"
      },
      "outputs": [],
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        In the constructor we instantiate two nn.Linear module\n",
        "        \"\"\"\n",
        "        super(Model, self).__init__()\n",
        "        self.l1 = nn.Linear(8, 6)\n",
        "        self.l2 = nn.Linear(6, 4)\n",
        "        self.l3 = nn.Linear(4, 1)\n",
        "\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        In the forward function we accept a Variable of input data and we must return\n",
        "        a Variable of output data. We can use Modules defined in the constructor as\n",
        "        well as arbitrary operators on Variables.\n",
        "        \"\"\"\n",
        "        out1 = self.sigmoid(self.l1(x))\n",
        "        out2 = self.sigmoid(self.l2(out1))\n",
        "        y_pred = self.sigmoid(self.l3(out2))\n",
        "        return y_pred\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b93fcc7-5fc7-430d-8c5a-ca8795723bdd",
      "metadata": {
        "id": "3b93fcc7-5fc7-430d-8c5a-ca8795723bdd"
      },
      "outputs": [],
      "source": [
        "# our model\n",
        "model = Model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b39b7456-2d02-4ce2-8437-9577b76c6f2c",
      "metadata": {
        "id": "b39b7456-2d02-4ce2-8437-9577b76c6f2c"
      },
      "outputs": [],
      "source": [
        "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
        "# in the SGD constructor will contain the learnable parameters of the two\n",
        "# nn.Linear modules which are members of the model.\n",
        "criterion = nn.BCELoss(reduction='mean')\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aea1f716-5573-4458-a021-b6efeceb379f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aea1f716-5573-4458-a021-b6efeceb379f",
        "outputId": "279b6850-5972-4bf6-e506-da10c345b10e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/100 | Loss: 0.6482\n",
            "Epoch: 2/100 | Loss: 0.6481\n",
            "Epoch: 3/100 | Loss: 0.6480\n",
            "Epoch: 4/100 | Loss: 0.6479\n",
            "Epoch: 5/100 | Loss: 0.6479\n",
            "Epoch: 6/100 | Loss: 0.6478\n",
            "Epoch: 7/100 | Loss: 0.6477\n",
            "Epoch: 8/100 | Loss: 0.6476\n",
            "Epoch: 9/100 | Loss: 0.6476\n",
            "Epoch: 10/100 | Loss: 0.6475\n",
            "Epoch: 11/100 | Loss: 0.6475\n",
            "Epoch: 12/100 | Loss: 0.6474\n",
            "Epoch: 13/100 | Loss: 0.6474\n",
            "Epoch: 14/100 | Loss: 0.6474\n",
            "Epoch: 15/100 | Loss: 0.6473\n",
            "Epoch: 16/100 | Loss: 0.6473\n",
            "Epoch: 17/100 | Loss: 0.6473\n",
            "Epoch: 18/100 | Loss: 0.6472\n",
            "Epoch: 19/100 | Loss: 0.6472\n",
            "Epoch: 20/100 | Loss: 0.6472\n",
            "Epoch: 21/100 | Loss: 0.6472\n",
            "Epoch: 22/100 | Loss: 0.6472\n",
            "Epoch: 23/100 | Loss: 0.6471\n",
            "Epoch: 24/100 | Loss: 0.6471\n",
            "Epoch: 25/100 | Loss: 0.6471\n",
            "Epoch: 26/100 | Loss: 0.6471\n",
            "Epoch: 27/100 | Loss: 0.6471\n",
            "Epoch: 28/100 | Loss: 0.6471\n",
            "Epoch: 29/100 | Loss: 0.6471\n",
            "Epoch: 30/100 | Loss: 0.6471\n",
            "Epoch: 31/100 | Loss: 0.6470\n",
            "Epoch: 32/100 | Loss: 0.6470\n",
            "Epoch: 33/100 | Loss: 0.6470\n",
            "Epoch: 34/100 | Loss: 0.6470\n",
            "Epoch: 35/100 | Loss: 0.6470\n",
            "Epoch: 36/100 | Loss: 0.6470\n",
            "Epoch: 37/100 | Loss: 0.6470\n",
            "Epoch: 38/100 | Loss: 0.6470\n",
            "Epoch: 39/100 | Loss: 0.6470\n",
            "Epoch: 40/100 | Loss: 0.6470\n",
            "Epoch: 41/100 | Loss: 0.6470\n",
            "Epoch: 42/100 | Loss: 0.6470\n",
            "Epoch: 43/100 | Loss: 0.6470\n",
            "Epoch: 44/100 | Loss: 0.6470\n",
            "Epoch: 45/100 | Loss: 0.6470\n",
            "Epoch: 46/100 | Loss: 0.6469\n",
            "Epoch: 47/100 | Loss: 0.6469\n",
            "Epoch: 48/100 | Loss: 0.6469\n",
            "Epoch: 49/100 | Loss: 0.6469\n",
            "Epoch: 50/100 | Loss: 0.6469\n",
            "Epoch: 51/100 | Loss: 0.6469\n",
            "Epoch: 52/100 | Loss: 0.6469\n",
            "Epoch: 53/100 | Loss: 0.6469\n",
            "Epoch: 54/100 | Loss: 0.6469\n",
            "Epoch: 55/100 | Loss: 0.6469\n",
            "Epoch: 56/100 | Loss: 0.6469\n",
            "Epoch: 57/100 | Loss: 0.6469\n",
            "Epoch: 58/100 | Loss: 0.6469\n",
            "Epoch: 59/100 | Loss: 0.6469\n",
            "Epoch: 60/100 | Loss: 0.6469\n",
            "Epoch: 61/100 | Loss: 0.6469\n",
            "Epoch: 62/100 | Loss: 0.6469\n",
            "Epoch: 63/100 | Loss: 0.6469\n",
            "Epoch: 64/100 | Loss: 0.6469\n",
            "Epoch: 65/100 | Loss: 0.6469\n",
            "Epoch: 66/100 | Loss: 0.6469\n",
            "Epoch: 67/100 | Loss: 0.6469\n",
            "Epoch: 68/100 | Loss: 0.6469\n",
            "Epoch: 69/100 | Loss: 0.6469\n",
            "Epoch: 70/100 | Loss: 0.6469\n",
            "Epoch: 71/100 | Loss: 0.6469\n",
            "Epoch: 72/100 | Loss: 0.6469\n",
            "Epoch: 73/100 | Loss: 0.6469\n",
            "Epoch: 74/100 | Loss: 0.6469\n",
            "Epoch: 75/100 | Loss: 0.6469\n",
            "Epoch: 76/100 | Loss: 0.6469\n",
            "Epoch: 77/100 | Loss: 0.6469\n",
            "Epoch: 78/100 | Loss: 0.6469\n",
            "Epoch: 79/100 | Loss: 0.6469\n",
            "Epoch: 80/100 | Loss: 0.6468\n",
            "Epoch: 81/100 | Loss: 0.6468\n",
            "Epoch: 82/100 | Loss: 0.6468\n",
            "Epoch: 83/100 | Loss: 0.6468\n",
            "Epoch: 84/100 | Loss: 0.6468\n",
            "Epoch: 85/100 | Loss: 0.6468\n",
            "Epoch: 86/100 | Loss: 0.6468\n",
            "Epoch: 87/100 | Loss: 0.6468\n",
            "Epoch: 88/100 | Loss: 0.6468\n",
            "Epoch: 89/100 | Loss: 0.6468\n",
            "Epoch: 90/100 | Loss: 0.6468\n",
            "Epoch: 91/100 | Loss: 0.6468\n",
            "Epoch: 92/100 | Loss: 0.6468\n",
            "Epoch: 93/100 | Loss: 0.6468\n",
            "Epoch: 94/100 | Loss: 0.6468\n",
            "Epoch: 95/100 | Loss: 0.6468\n",
            "Epoch: 96/100 | Loss: 0.6468\n",
            "Epoch: 97/100 | Loss: 0.6468\n",
            "Epoch: 98/100 | Loss: 0.6468\n",
            "Epoch: 99/100 | Loss: 0.6468\n",
            "Epoch: 100/100 | Loss: 0.6468\n"
          ]
        }
      ],
      "source": [
        "# Training loop\n",
        "for epoch in range(100):\n",
        "    # Forward pass: Compute predicted y by passing x to the model\n",
        "    y_pred = model(x_data)\n",
        "\n",
        "    # Compute and print loss\n",
        "    loss = criterion(y_pred, y_data)\n",
        "    print(f'Epoch: {epoch + 1}/100 | Loss: {loss.item():.4f}')\n",
        "\n",
        "    # Zero gradients, perform a backward pass, and update the weights.\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96a3e477-b942-445a-913b-910233cbba39",
      "metadata": {
        "id": "96a3e477-b942-445a-913b-910233cbba39"
      },
      "source": [
        "## Lecture 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7163e32-fec6-47c9-8af0-8635daca84ab",
      "metadata": {
        "id": "c7163e32-fec6-47c9-8af0-8635daca84ab"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch import from_numpy, tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56b5b7dd-1865-4280-94f9-19ab7149ebe5",
      "metadata": {
        "id": "56b5b7dd-1865-4280-94f9-19ab7149ebe5"
      },
      "outputs": [],
      "source": [
        "class DiabetesDataset(Dataset):\n",
        "    \"\"\" Diabetes dataset.\"\"\"\n",
        "\n",
        "    # Initialize your data, download, etc.\n",
        "    def __init__(self):\n",
        "        xy = np.loadtxt('./diabetes.csv',\n",
        "                        delimiter=',', dtype=np.float32, skiprows=1)\n",
        "        self.len = xy.shape[0]\n",
        "        self.x_data = from_numpy(xy[:, 0:-1])\n",
        "        self.y_data = from_numpy(xy[:, [-1]])\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.x_data[index], self.y_data[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74643cdb-e616-436b-9a16-0356227f9cd5",
      "metadata": {
        "id": "74643cdb-e616-436b-9a16-0356227f9cd5"
      },
      "outputs": [],
      "source": [
        "dataset = DiabetesDataset()\n",
        "train_loader = DataLoader(dataset=dataset,\n",
        "                          batch_size=32,\n",
        "                          shuffle=True,\n",
        "                          num_workers=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bdf30c2c-918b-450f-b38f-528f21ecad17",
      "metadata": {
        "id": "bdf30c2c-918b-450f-b38f-528f21ecad17"
      },
      "outputs": [],
      "source": [
        "class Model(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        In the constructor we instantiate two nn.Linear module\n",
        "        \"\"\"\n",
        "        super(Model, self).__init__()\n",
        "        self.l1 = nn.Linear(8, 6)\n",
        "        self.l2 = nn.Linear(6, 4)\n",
        "        self.l3 = nn.Linear(4, 1)\n",
        "\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        In the forward function we accept a Variable of input data and we must return\n",
        "        a Variable of output data. We can use Modules defined in the constructor as\n",
        "        well as arbitrary operators on Variables.\n",
        "        \"\"\"\n",
        "        out1 = self.sigmoid(self.l1(x))\n",
        "        out2 = self.sigmoid(self.l2(out1))\n",
        "        y_pred = self.sigmoid(self.l3(out2))\n",
        "        return y_pred\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f067976-c58f-4bff-84e9-e7b2ede2b6a8",
      "metadata": {
        "id": "8f067976-c58f-4bff-84e9-e7b2ede2b6a8"
      },
      "outputs": [],
      "source": [
        "# our model\n",
        "model = Model()\n",
        "\n",
        "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
        "# in the SGD constructor will contain the learnable parameters of the two\n",
        "# nn.Linear modules which are members of the model.\n",
        "criterion = nn.BCELoss(reduction='sum')\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "261a9256-92dc-4d81-9dcf-700aa606d4ef",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "261a9256-92dc-4d81-9dcf-700aa606d4ef",
        "outputId": "8fe481a6-68c0-4362-a160-57480bf3aabe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | Batch: 1 | Loss: 23.4689\n",
            "Epoch 1 | Batch: 2 | Loss: 23.4857\n",
            "Epoch 1 | Batch: 3 | Loss: 21.2603\n",
            "Epoch 1 | Batch: 4 | Loss: 20.6073\n",
            "Epoch 1 | Batch: 5 | Loss: 20.5396\n",
            "Epoch 1 | Batch: 6 | Loss: 19.9098\n",
            "Epoch 1 | Batch: 7 | Loss: 24.1029\n",
            "Epoch 1 | Batch: 8 | Loss: 23.6846\n",
            "Epoch 1 | Batch: 9 | Loss: 22.4346\n",
            "Epoch 1 | Batch: 10 | Loss: 20.9846\n",
            "Epoch 1 | Batch: 11 | Loss: 21.3123\n",
            "Epoch 1 | Batch: 12 | Loss: 20.2965\n",
            "Epoch 1 | Batch: 13 | Loss: 18.8908\n",
            "Epoch 1 | Batch: 14 | Loss: 22.0517\n",
            "Epoch 1 | Batch: 15 | Loss: 20.1691\n",
            "Epoch 1 | Batch: 16 | Loss: 17.9731\n",
            "Epoch 1 | Batch: 17 | Loss: 21.3382\n",
            "Epoch 1 | Batch: 18 | Loss: 21.1533\n",
            "Epoch 1 | Batch: 19 | Loss: 21.1700\n",
            "Epoch 1 | Batch: 20 | Loss: 22.7182\n",
            "Epoch 1 | Batch: 21 | Loss: 22.1577\n",
            "Epoch 1 | Batch: 22 | Loss: 20.1800\n",
            "Epoch 1 | Batch: 23 | Loss: 19.8898\n",
            "Epoch 1 | Batch: 24 | Loss: 20.6394\n",
            "Epoch 2 | Batch: 1 | Loss: 20.5945\n",
            "Epoch 2 | Batch: 2 | Loss: 18.6322\n",
            "Epoch 2 | Batch: 3 | Loss: 19.1790\n",
            "Epoch 2 | Batch: 4 | Loss: 24.4601\n",
            "Epoch 2 | Batch: 5 | Loss: 22.4009\n",
            "Epoch 2 | Batch: 6 | Loss: 19.7552\n",
            "Epoch 2 | Batch: 7 | Loss: 23.0043\n",
            "Epoch 2 | Batch: 8 | Loss: 21.0804\n",
            "Epoch 2 | Batch: 9 | Loss: 15.7355\n",
            "Epoch 2 | Batch: 10 | Loss: 28.0227\n",
            "Epoch 2 | Batch: 11 | Loss: 21.9782\n",
            "Epoch 2 | Batch: 12 | Loss: 22.4247\n",
            "Epoch 2 | Batch: 13 | Loss: 22.2376\n",
            "Epoch 2 | Batch: 14 | Loss: 21.8995\n",
            "Epoch 2 | Batch: 15 | Loss: 21.2339\n",
            "Epoch 2 | Batch: 16 | Loss: 19.6401\n",
            "Epoch 2 | Batch: 17 | Loss: 22.6515\n",
            "Epoch 2 | Batch: 18 | Loss: 20.8237\n",
            "Epoch 2 | Batch: 19 | Loss: 20.5920\n",
            "Epoch 2 | Batch: 20 | Loss: 20.5948\n",
            "Epoch 2 | Batch: 21 | Loss: 21.2376\n",
            "Epoch 2 | Batch: 22 | Loss: 18.5897\n",
            "Epoch 2 | Batch: 23 | Loss: 16.8420\n",
            "Epoch 2 | Batch: 24 | Loss: 26.8093\n"
          ]
        }
      ],
      "source": [
        "# Training loop\n",
        "for epoch in range(2):\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        # get the inputs\n",
        "        inputs, labels = data\n",
        "\n",
        "        # Forward pass: Compute predicted y by passing x to the model\n",
        "        y_pred = model(inputs)\n",
        "\n",
        "        # Compute and print loss\n",
        "        loss = criterion(y_pred, labels)\n",
        "        print(f'Epoch {epoch + 1} | Batch: {i+1} | Loss: {loss.item():.4f}')\n",
        "\n",
        "        # Zero gradients, perform a backward pass, and update the weights.\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04f34d40-de3c-460a-a43f-8f936690ae23",
      "metadata": {
        "id": "04f34d40-de3c-460a-a43f-8f936690ae23"
      },
      "source": [
        "## Lecture 9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6965d1e-965c-4465-8589-d7f7793eb8d9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6965d1e-965c-4465-8589-d7f7793eb8d9",
        "outputId": "d9f9e77f-3fe7-4e2d-9035-50db510c1ad4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training MNIST Model on cpu\n",
            "============================================\n"
          ]
        }
      ],
      "source": [
        "# Training settings\n",
        "import torch\n",
        "batch_size = 64\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f'Training MNIST Model on {device}\\n{\"=\" * 44}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a3437ad-c399-43e9-92ef-65d72ca4bd1b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "8a3437ad-c399-43e9-92ef-65d72ca4bd1b",
        "outputId": "bb3c9f17-b78a-499e-f5ef-fee85bb878b0"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'datasets' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-3c348baa0df1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m train_dataset = datasets.MNIST(root='./mnist_data/',\n\u001b[0m\u001b[1;32m      2\u001b[0m                                \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                download=True)\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'datasets' is not defined"
          ]
        }
      ],
      "source": [
        "\n",
        "train_dataset = datasets.MNIST(root='./mnist_data/',\n",
        "                               train=True,\n",
        "                               transform=transforms.ToTensor(),\n",
        "                               download=True)\n",
        "\n",
        "test_dataset = datasets.MNIST(root='./mnist_data/',\n",
        "                              train=False,\n",
        "                              transform=transforms.ToTensor())\n",
        "\n",
        "# Data Loader (Input Pipeline)\n",
        "train_loader = data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=batch_size,\n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = data.DataLoader(dataset=test_dataset,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87dea7d0-1415-4127-89ec-37801cb91263",
      "metadata": {
        "id": "87dea7d0-1415-4127-89ec-37801cb91263"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.l1 = nn.Linear(784, 520)\n",
        "        self.l2 = nn.Linear(520, 320)\n",
        "        self.l3 = nn.Linear(320, 240)\n",
        "        self.l4 = nn.Linear(240, 120)\n",
        "        self.l5 = nn.Linear(120, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Print the shape of the input tensor before reshaping\n",
        "        print(\"Shape of input tensor before view:\", x.shape)\n",
        "        x = x.view(-1, 784)  # Flatten the data (n, 1, 28, 28)-> (n, 784)\n",
        "        x = F.relu(self.l1(x))\n",
        "        x = F.relu(self.l2(x))\n",
        "        x = F.relu(self.l3(x))\n",
        "        x = F.relu(self.l4(x))\n",
        "        return self.l5(x)\n",
        "\n",
        "model = Net()\n",
        "model.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 10 == 0:\n",
        "            print('Train Epoch: {} | Batch Status: {}/{} ({:.0f}%) | Loss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n"
      ],
      "metadata": {
        "id": "IlR_8GD8BL5Q"
      },
      "id": "IlR_8GD8BL5Q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test():\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    for data, target in test_loader:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        output = model(data)\n",
        "        # sum up batch loss\n",
        "        test_loss += criterion(output, target).item()\n",
        "        # get the index of the max\n",
        "        pred = output.data.max(1, keepdim=True)[1]\n",
        "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print(f'===========================\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} '\n",
        "          f'({100. * correct / len(test_loader.dataset):.0f}%)')\n"
      ],
      "metadata": {
        "id": "YwRdE5SvBSRh"
      },
      "id": "YwRdE5SvBSRh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "if __name__ == '__main__':\n",
        "    since = time.time()\n",
        "    for epoch in range(1, 10):\n",
        "        epoch_start = time.time()\n",
        "        train(epoch)\n",
        "        m, s = divmod(time.time() - epoch_start, 60)\n",
        "        print(f'Training time: {m:.0f}m {s:.0f}s')\n",
        "        test()\n",
        "        m, s = divmod(time.time() - epoch_start, 60)\n",
        "        print(f'Testing time: {m:.0f}m {s:.0f}s')\n",
        "\n",
        "    m, s = divmod(time.time() - since, 60)\n",
        "    print(f'Total Time: {m:.0f}m {s:.0f}s\\nModel was trained on {device}!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "8lJeMsEhBSW_",
        "outputId": "b7f8df1c-932b-40ec-b2e2-8db723d43709"
      },
      "id": "8lJeMsEhBSW_",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of input tensor before view: torch.Size([32, 8])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "shape '[-1, 784]' is invalid for input of size 256",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-0eb3a07af130>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mepoch_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdivmod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mepoch_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Training time: {m:.0f}m {s:.0f}s'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-50-1d0fbdefeb66>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-49-7a3add7219ec>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m# Print the shape of the input tensor before reshaping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Shape of input tensor before view:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m784\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Flatten the data (n, 1, 28, 28)-> (n, 784)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: shape '[-1, 784]' is invalid for input of size 256"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lecture 10"
      ],
      "metadata": {
        "id": "h384nsk0BdPv"
      },
      "id": "h384nsk0BdPv"
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import argparse\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.autograd import Variable"
      ],
      "metadata": {
        "id": "Aj2VV6QaBe6N"
      },
      "id": "Aj2VV6QaBe6N",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training settings\n",
        "batch_size = 64\n",
        "\n",
        "# MNIST Dataset\n",
        "train_dataset = datasets.MNIST(root='./data/',\n",
        "                               train=True,\n",
        "                               transform=transforms.ToTensor(),\n",
        "                               download=True)\n",
        "\n",
        "test_dataset = datasets.MNIST(root='./data/',\n",
        "                              train=False,\n",
        "                              transform=transforms.ToTensor())\n",
        "\n",
        "# Data Loader (Input Pipeline)\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=batch_size,\n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bh3CGfgRCU08",
        "outputId": "9cb9277b-aaec-4fe7-ab37-cc47014dd1f2"
      },
      "id": "Bh3CGfgRCU08",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 121MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 34.7MB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 95.2MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 2.46MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training settings\n",
        "batch_size = 64\n",
        "\n",
        "# MNIST Dataset\n",
        "train_dataset = datasets.MNIST(root='./data/',\n",
        "                               train=True,\n",
        "                               transform=transforms.ToTensor(),\n",
        "                               download=True)\n",
        "\n",
        "test_dataset = datasets.MNIST(root='./data/',\n",
        "                              train=False,\n",
        "                              transform=transforms.ToTensor())\n",
        "\n",
        "# Data Loader (Input Pipeline)\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=batch_size,\n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=False)\n"
      ],
      "metadata": {
        "id": "o1G9Puv9CXqT"
      },
      "id": "o1G9Puv9CXqT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Net()\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)"
      ],
      "metadata": {
        "id": "_6PhNKtcCZ1G"
      },
      "id": "_6PhNKtcCZ1G",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = Variable(data), Variable(target)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 10 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n"
      ],
      "metadata": {
        "id": "_HBzWBmzCb4o"
      },
      "id": "_HBzWBmzCb4o",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test():\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    for data, target in test_loader:\n",
        "        data, target = Variable(data, volatile=True), Variable(target)\n",
        "        output = model(data)\n",
        "        # sum up batch loss\n",
        "        test_loss += F.nll_loss(output, target, size_average=False).data\n",
        "        # get the index of the max log-probability\n",
        "        pred = output.data.max(1, keepdim=True)[1]\n",
        "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n"
      ],
      "metadata": {
        "id": "wXPtqCCTCdXI"
      },
      "id": "wXPtqCCTCdXI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test():\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    for data, target in test_loader:\n",
        "        data, target = Variable(data, volatile=True), Variable(target)\n",
        "        output = model(data)\n",
        "        # sum up batch loss\n",
        "        test_loss += F.nll_loss(output, target, size_average=False).data\n",
        "        # get the index of the max log-probability\n",
        "        pred = output.data.max(1, keepdim=True)[1]\n",
        "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n"
      ],
      "metadata": {
        "id": "CeeAnnLMCfFN"
      },
      "id": "CeeAnnLMCfFN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JBF7OaJRCgc1"
      },
      "id": "JBF7OaJRCgc1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lecture 11"
      ],
      "metadata": {
        "id": "EErpokgHCi4K"
      },
      "id": "EErpokgHCi4K"
    },
    {
      "cell_type": "code",
      "source": [
        "class InceptionA(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels):\n",
        "        super(InceptionA, self).__init__()\n",
        "        self.branch1x1 = nn.Conv2d(in_channels, 16, kernel_size=1)\n",
        "\n",
        "        self.branch5x5_1 = nn.Conv2d(in_channels, 16, kernel_size=1)\n",
        "        self.branch5x5_2 = nn.Conv2d(16, 24, kernel_size=5, padding=2)\n",
        "\n",
        "        self.branch3x3dbl_1 = nn.Conv2d(in_channels, 16, kernel_size=1)\n",
        "        self.branch3x3dbl_2 = nn.Conv2d(16, 24, kernel_size=3, padding=1)\n",
        "        self.branch3x3dbl_3 = nn.Conv2d(24, 24, kernel_size=3, padding=1)\n",
        "\n",
        "        self.branch_pool = nn.Conv2d(in_channels, 24, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        branch1x1 = self.branch1x1(x)\n",
        "\n",
        "        branch5x5 = self.branch5x5_1(x)\n",
        "        branch5x5 = self.branch5x5_2(branch5x5)\n",
        "\n",
        "        branch3x3dbl = self.branch3x3dbl_1(x)\n",
        "        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n",
        "        branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)\n",
        "\n",
        "        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)\n",
        "        branch_pool = self.branch_pool(branch_pool)\n",
        "\n",
        "        outputs = [branch1x1, branch5x5, branch3x3dbl, branch_pool]\n",
        "        return torch.cat(outputs, 1)\n",
        "\n"
      ],
      "metadata": {
        "id": "p0nhUCIfCk_S"
      },
      "id": "p0nhUCIfCk_S",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(88, 20, kernel_size=5)\n",
        "\n",
        "        self.incept1 = InceptionA(in_channels=10)\n",
        "        self.incept2 = InceptionA(in_channels=20)\n",
        "\n",
        "        self.mp = nn.MaxPool2d(2)\n",
        "        self.fc = nn.Linear(1408, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        in_size = x.size(0)\n",
        "        x = F.relu(self.mp(self.conv1(x)))\n",
        "        x = self.incept1(x)\n",
        "        x = F.relu(self.mp(self.conv2(x)))\n",
        "        x = self.incept2(x)\n",
        "        x = x.view(in_size, -1)  # flatten the tensor\n",
        "        x = self.fc(x)\n",
        "        return F.log_softmax(x)\n"
      ],
      "metadata": {
        "id": "FV5EsSDuCyyi"
      },
      "id": "FV5EsSDuCyyi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Net()\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n"
      ],
      "metadata": {
        "id": "nke0EpA6C0rj"
      },
      "id": "nke0EpA6C0rj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = Variable(data), Variable(target)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 10 == 0:\n",
        "            # Use .item() to get the scalar value of the loss\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n"
      ],
      "metadata": {
        "id": "nNwlQ-2oC3He"
      },
      "id": "nNwlQ-2oC3He",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test():\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    for data, target in test_loader:\n",
        "        # Setting volatile=True is deprecated. Instead, use torch.no_grad()\n",
        "        # data, target = Variable(data, volatile=True), Variable(target)\n",
        "        with torch.no_grad(): # Use torch.no_grad() for inference\n",
        "            data, target = Variable(data), Variable(target)\n",
        "            output = model(data)\n",
        "            # sum up batch loss\n",
        "            # Use .item() to get the scalar value of the loss\n",
        "            test_loss += F.nll_loss(output, target, size_average=False).item()\n",
        "            # get the index of the max log-probability\n",
        "            pred = output.data.max(1, keepdim=True)[1]\n",
        "            correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))"
      ],
      "metadata": {
        "id": "TZuKqCM5C5F0"
      },
      "id": "TZuKqCM5C5F0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1, 10):\n",
        "    train(epoch)\n",
        "    test()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmkn9c8PC6sk",
        "outputId": "ddb1aefd-9d8a-48d9-e9e6-ff08231b5b47"
      },
      "id": "vmkn9c8PC6sk",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-61-6e735d0660b5>:22: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return F.log_softmax(x)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.164842\n",
            "Train Epoch: 1 [640/60000 (1%)]\tLoss: 0.124293\n",
            "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 0.099701\n",
            "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 0.159447\n",
            "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 0.154275\n",
            "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 0.333550\n",
            "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 0.156520\n",
            "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 0.101680\n",
            "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 0.124076\n",
            "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 0.274023\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.117843\n",
            "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 0.167639\n",
            "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 0.197208\n",
            "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 0.117625\n",
            "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 0.184003\n",
            "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.176402\n",
            "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.149364\n",
            "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 0.184832\n",
            "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 0.220281\n",
            "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 0.150813\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.269633\n",
            "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 0.267150\n",
            "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.167534\n",
            "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 0.041304\n",
            "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.176901\n",
            "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.084829\n",
            "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.258446\n",
            "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.104528\n",
            "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.043871\n",
            "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.096327\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.140235\n",
            "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.128923\n",
            "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.186838\n",
            "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.260235\n",
            "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.097206\n",
            "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.117937\n",
            "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.067880\n",
            "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.224584\n",
            "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.143112\n",
            "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.198151\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.062202\n",
            "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.219038\n",
            "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.113527\n",
            "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.111102\n",
            "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.254555\n",
            "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.131574\n",
            "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.205014\n",
            "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.056657\n",
            "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.041255\n",
            "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.366548\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.049300\n",
            "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.081056\n",
            "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.178795\n",
            "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.188601\n",
            "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.131103\n",
            "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.115240\n",
            "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.130227\n",
            "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.056315\n",
            "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.044336\n",
            "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.066364\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.101528\n",
            "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.195238\n",
            "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.122486\n",
            "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.091732\n",
            "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.230690\n",
            "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.032871\n",
            "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.094224\n",
            "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.052670\n",
            "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.057762\n",
            "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.030709\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.057511\n",
            "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.150734\n",
            "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.040394\n",
            "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.022237\n",
            "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.116208\n",
            "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.060970\n",
            "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.060799\n",
            "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.065264\n",
            "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.044554\n",
            "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.077914\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.049512\n",
            "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.118867\n",
            "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.106289\n",
            "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.220789\n",
            "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.169050\n",
            "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.114535\n",
            "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.083643\n",
            "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.039940\n",
            "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.048768\n",
            "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.116494\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.164338\n",
            "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.125020\n",
            "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.067604\n",
            "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.058651\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/_reduction.py:51: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0967, Accuracy: 9706/10000 (97%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.025479\n",
            "Train Epoch: 2 [640/60000 (1%)]\tLoss: 0.148593\n",
            "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.060727\n",
            "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 0.056925\n",
            "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.018889\n",
            "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.169727\n",
            "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.175438\n",
            "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 0.048143\n",
            "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.106909\n",
            "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 0.193090\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.051633\n",
            "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 0.145911\n",
            "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.067931\n",
            "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.239918\n",
            "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.060757\n",
            "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.048062\n",
            "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.072358\n",
            "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.067415\n",
            "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.086207\n",
            "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.202487\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.091179\n",
            "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.088967\n",
            "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.127043\n",
            "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.082137\n",
            "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.119019\n",
            "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.050857\n",
            "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.062533\n",
            "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.040042\n",
            "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.051622\n",
            "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.162715\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.058071\n",
            "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.077001\n",
            "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.060817\n",
            "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.096298\n",
            "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.041942\n",
            "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.144570\n",
            "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.092264\n",
            "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.060379\n",
            "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.032489\n",
            "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.082347\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.071551\n",
            "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.039281\n",
            "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.159378\n",
            "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.072898\n",
            "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.196780\n",
            "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.052929\n",
            "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.039116\n",
            "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.063300\n",
            "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.086385\n",
            "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.142953\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.070296\n",
            "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.073700\n",
            "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.080921\n",
            "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.032350\n",
            "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.255601\n",
            "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.111939\n",
            "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.142476\n",
            "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.049787\n",
            "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.093300\n",
            "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.092281\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.030834\n",
            "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.019957\n",
            "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.158913\n",
            "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.156305\n",
            "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.122476\n",
            "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.043428\n",
            "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.039330\n",
            "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.100294\n",
            "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.112830\n",
            "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.164303\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.117953\n",
            "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.081472\n",
            "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.014006\n",
            "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.121068\n",
            "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.065995\n",
            "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.036080\n",
            "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.037069\n",
            "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.128125\n",
            "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.111808\n",
            "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.084458\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.050864\n",
            "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.019558\n",
            "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.201349\n",
            "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.104504\n",
            "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.063226\n",
            "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.100331\n",
            "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.117168\n",
            "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.187807\n",
            "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.161286\n",
            "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.109704\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.022600\n",
            "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.040847\n",
            "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.023901\n",
            "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.217923\n",
            "\n",
            "Test set: Average loss: 0.0812, Accuracy: 9729/10000 (97%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.058013\n",
            "Train Epoch: 3 [640/60000 (1%)]\tLoss: 0.175205\n",
            "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.019133\n",
            "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 0.064833\n",
            "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.059842\n",
            "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.072184\n",
            "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.077676\n",
            "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.135669\n",
            "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.034667\n",
            "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 0.066360\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.153995\n",
            "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 0.102665\n",
            "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.222345\n",
            "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 0.119429\n",
            "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.085823\n",
            "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.224956\n",
            "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.148468\n",
            "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 0.076628\n",
            "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.025335\n",
            "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 0.041121\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.122168\n",
            "Train Epoch: 3 [13440/60000 (22%)]\tLoss: 0.040528\n",
            "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.188320\n",
            "Train Epoch: 3 [14720/60000 (25%)]\tLoss: 0.017705\n",
            "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.042947\n",
            "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.038210\n",
            "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.119856\n",
            "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 0.079435\n",
            "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.036559\n",
            "Train Epoch: 3 [18560/60000 (31%)]\tLoss: 0.084214\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.026694\n",
            "Train Epoch: 3 [19840/60000 (33%)]\tLoss: 0.090861\n",
            "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.232434\n",
            "Train Epoch: 3 [21120/60000 (35%)]\tLoss: 0.041654\n",
            "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.126692\n",
            "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.079977\n",
            "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.033109\n",
            "Train Epoch: 3 [23680/60000 (39%)]\tLoss: 0.049583\n",
            "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.046733\n",
            "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.137941\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.040584\n",
            "Train Epoch: 3 [26240/60000 (44%)]\tLoss: 0.051473\n",
            "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.042110\n",
            "Train Epoch: 3 [27520/60000 (46%)]\tLoss: 0.078211\n",
            "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.165248\n",
            "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.008771\n",
            "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.039144\n",
            "Train Epoch: 3 [30080/60000 (50%)]\tLoss: 0.027365\n",
            "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.060059\n",
            "Train Epoch: 3 [31360/60000 (52%)]\tLoss: 0.063507\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.043940\n",
            "Train Epoch: 3 [32640/60000 (54%)]\tLoss: 0.055298\n",
            "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.030514\n",
            "Train Epoch: 3 [33920/60000 (57%)]\tLoss: 0.112088\n",
            "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.049014\n",
            "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.118226\n",
            "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.130741\n",
            "Train Epoch: 3 [36480/60000 (61%)]\tLoss: 0.090196\n",
            "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.048540\n",
            "Train Epoch: 3 [37760/60000 (63%)]\tLoss: 0.090433\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.066221\n",
            "Train Epoch: 3 [39040/60000 (65%)]\tLoss: 0.067486\n",
            "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.140069\n",
            "Train Epoch: 3 [40320/60000 (67%)]\tLoss: 0.060640\n",
            "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.011604\n",
            "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.096226\n",
            "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.098167\n",
            "Train Epoch: 3 [42880/60000 (71%)]\tLoss: 0.036963\n",
            "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.024719\n",
            "Train Epoch: 3 [44160/60000 (74%)]\tLoss: 0.079353\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.034559\n",
            "Train Epoch: 3 [45440/60000 (76%)]\tLoss: 0.163435\n",
            "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.139900\n",
            "Train Epoch: 3 [46720/60000 (78%)]\tLoss: 0.073646\n",
            "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.133744\n",
            "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.063283\n",
            "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.132634\n",
            "Train Epoch: 3 [49280/60000 (82%)]\tLoss: 0.028950\n",
            "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.043881\n",
            "Train Epoch: 3 [50560/60000 (84%)]\tLoss: 0.147586\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.055405\n",
            "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.066496\n",
            "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.152863\n",
            "Train Epoch: 3 [53120/60000 (88%)]\tLoss: 0.105730\n",
            "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.094515\n",
            "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.079970\n",
            "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.052414\n",
            "Train Epoch: 3 [55680/60000 (93%)]\tLoss: 0.056365\n",
            "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.027841\n",
            "Train Epoch: 3 [56960/60000 (95%)]\tLoss: 0.087716\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.074118\n",
            "Train Epoch: 3 [58240/60000 (97%)]\tLoss: 0.089846\n",
            "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.080334\n",
            "Train Epoch: 3 [59520/60000 (99%)]\tLoss: 0.090601\n",
            "\n",
            "Test set: Average loss: 0.0646, Accuracy: 9792/10000 (98%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.030871\n",
            "Train Epoch: 4 [640/60000 (1%)]\tLoss: 0.033098\n",
            "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 0.031612\n",
            "Train Epoch: 4 [1920/60000 (3%)]\tLoss: 0.096313\n",
            "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 0.085898\n",
            "Train Epoch: 4 [3200/60000 (5%)]\tLoss: 0.039029\n",
            "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 0.062328\n",
            "Train Epoch: 4 [4480/60000 (7%)]\tLoss: 0.228221\n",
            "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 0.258292\n",
            "Train Epoch: 4 [5760/60000 (10%)]\tLoss: 0.073143\n",
            "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.047240\n",
            "Train Epoch: 4 [7040/60000 (12%)]\tLoss: 0.027170\n",
            "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 0.169786\n",
            "Train Epoch: 4 [8320/60000 (14%)]\tLoss: 0.161227\n",
            "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 0.195637\n",
            "Train Epoch: 4 [9600/60000 (16%)]\tLoss: 0.065472\n",
            "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.071918\n",
            "Train Epoch: 4 [10880/60000 (18%)]\tLoss: 0.118011\n",
            "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 0.051825\n",
            "Train Epoch: 4 [12160/60000 (20%)]\tLoss: 0.055223\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.017013\n",
            "Train Epoch: 4 [13440/60000 (22%)]\tLoss: 0.003992\n",
            "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 0.093119\n",
            "Train Epoch: 4 [14720/60000 (25%)]\tLoss: 0.016235\n",
            "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 0.051103\n",
            "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.145060\n",
            "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 0.143606\n",
            "Train Epoch: 4 [17280/60000 (29%)]\tLoss: 0.114155\n",
            "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 0.046213\n",
            "Train Epoch: 4 [18560/60000 (31%)]\tLoss: 0.026742\n",
            "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.056790\n",
            "Train Epoch: 4 [19840/60000 (33%)]\tLoss: 0.023499\n",
            "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.017626\n",
            "Train Epoch: 4 [21120/60000 (35%)]\tLoss: 0.174490\n",
            "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 0.050119\n",
            "Train Epoch: 4 [22400/60000 (37%)]\tLoss: 0.133091\n",
            "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 0.114531\n",
            "Train Epoch: 4 [23680/60000 (39%)]\tLoss: 0.045191\n",
            "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 0.061140\n",
            "Train Epoch: 4 [24960/60000 (42%)]\tLoss: 0.045130\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.024181\n",
            "Train Epoch: 4 [26240/60000 (44%)]\tLoss: 0.069831\n",
            "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 0.121765\n",
            "Train Epoch: 4 [27520/60000 (46%)]\tLoss: 0.060698\n",
            "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 0.057657\n",
            "Train Epoch: 4 [28800/60000 (48%)]\tLoss: 0.016472\n",
            "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 0.084438\n",
            "Train Epoch: 4 [30080/60000 (50%)]\tLoss: 0.152826\n",
            "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.022361\n",
            "Train Epoch: 4 [31360/60000 (52%)]\tLoss: 0.070777\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.117595\n",
            "Train Epoch: 4 [32640/60000 (54%)]\tLoss: 0.061287\n",
            "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 0.044906\n",
            "Train Epoch: 4 [33920/60000 (57%)]\tLoss: 0.147215\n",
            "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 0.022483\n",
            "Train Epoch: 4 [35200/60000 (59%)]\tLoss: 0.123748\n",
            "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 0.017478\n",
            "Train Epoch: 4 [36480/60000 (61%)]\tLoss: 0.040401\n",
            "Train Epoch: 4 [37120/60000 (62%)]\tLoss: 0.074940\n",
            "Train Epoch: 4 [37760/60000 (63%)]\tLoss: 0.053404\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.019169\n",
            "Train Epoch: 4 [39040/60000 (65%)]\tLoss: 0.070155\n",
            "Train Epoch: 4 [39680/60000 (66%)]\tLoss: 0.315351\n",
            "Train Epoch: 4 [40320/60000 (67%)]\tLoss: 0.085058\n",
            "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.013779\n",
            "Train Epoch: 4 [41600/60000 (69%)]\tLoss: 0.120155\n",
            "Train Epoch: 4 [42240/60000 (70%)]\tLoss: 0.013874\n",
            "Train Epoch: 4 [42880/60000 (71%)]\tLoss: 0.056588\n",
            "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 0.086487\n",
            "Train Epoch: 4 [44160/60000 (74%)]\tLoss: 0.061670\n",
            "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.091694\n",
            "Train Epoch: 4 [45440/60000 (76%)]\tLoss: 0.024324\n",
            "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 0.067043\n",
            "Train Epoch: 4 [46720/60000 (78%)]\tLoss: 0.060999\n",
            "Train Epoch: 4 [47360/60000 (79%)]\tLoss: 0.040534\n",
            "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.005898\n",
            "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 0.030409\n",
            "Train Epoch: 4 [49280/60000 (82%)]\tLoss: 0.093078\n",
            "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 0.049852\n",
            "Train Epoch: 4 [50560/60000 (84%)]\tLoss: 0.145609\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.018281\n",
            "Train Epoch: 4 [51840/60000 (86%)]\tLoss: 0.089801\n",
            "Train Epoch: 4 [52480/60000 (87%)]\tLoss: 0.181512\n",
            "Train Epoch: 4 [53120/60000 (88%)]\tLoss: 0.046880\n",
            "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 0.009758\n",
            "Train Epoch: 4 [54400/60000 (91%)]\tLoss: 0.035140\n",
            "Train Epoch: 4 [55040/60000 (92%)]\tLoss: 0.113845\n",
            "Train Epoch: 4 [55680/60000 (93%)]\tLoss: 0.051951\n",
            "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 0.140697\n",
            "Train Epoch: 4 [56960/60000 (95%)]\tLoss: 0.078456\n",
            "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.022755\n",
            "Train Epoch: 4 [58240/60000 (97%)]\tLoss: 0.033987\n",
            "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.112668\n",
            "Train Epoch: 4 [59520/60000 (99%)]\tLoss: 0.044053\n",
            "\n",
            "Test set: Average loss: 0.0567, Accuracy: 9806/10000 (98%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.055883\n",
            "Train Epoch: 5 [640/60000 (1%)]\tLoss: 0.038997\n",
            "Train Epoch: 5 [1280/60000 (2%)]\tLoss: 0.018462\n",
            "Train Epoch: 5 [1920/60000 (3%)]\tLoss: 0.093564\n",
            "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 0.131875\n",
            "Train Epoch: 5 [3200/60000 (5%)]\tLoss: 0.021378\n",
            "Train Epoch: 5 [3840/60000 (6%)]\tLoss: 0.012789\n",
            "Train Epoch: 5 [4480/60000 (7%)]\tLoss: 0.004820\n",
            "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 0.128893\n",
            "Train Epoch: 5 [5760/60000 (10%)]\tLoss: 0.013707\n",
            "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.096819\n",
            "Train Epoch: 5 [7040/60000 (12%)]\tLoss: 0.174980\n",
            "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 0.152947\n",
            "Train Epoch: 5 [8320/60000 (14%)]\tLoss: 0.236665\n",
            "Train Epoch: 5 [8960/60000 (15%)]\tLoss: 0.062714\n",
            "Train Epoch: 5 [9600/60000 (16%)]\tLoss: 0.014951\n",
            "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.021111\n",
            "Train Epoch: 5 [10880/60000 (18%)]\tLoss: 0.051638\n",
            "Train Epoch: 5 [11520/60000 (19%)]\tLoss: 0.036083\n",
            "Train Epoch: 5 [12160/60000 (20%)]\tLoss: 0.030450\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.059688\n",
            "Train Epoch: 5 [13440/60000 (22%)]\tLoss: 0.080772\n",
            "Train Epoch: 5 [14080/60000 (23%)]\tLoss: 0.054111\n",
            "Train Epoch: 5 [14720/60000 (25%)]\tLoss: 0.110866\n",
            "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 0.059963\n",
            "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.080531\n",
            "Train Epoch: 5 [16640/60000 (28%)]\tLoss: 0.014032\n",
            "Train Epoch: 5 [17280/60000 (29%)]\tLoss: 0.034383\n",
            "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 0.054127\n",
            "Train Epoch: 5 [18560/60000 (31%)]\tLoss: 0.044294\n",
            "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.034306\n",
            "Train Epoch: 5 [19840/60000 (33%)]\tLoss: 0.067430\n",
            "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.013217\n",
            "Train Epoch: 5 [21120/60000 (35%)]\tLoss: 0.036003\n",
            "Train Epoch: 5 [21760/60000 (36%)]\tLoss: 0.094902\n",
            "Train Epoch: 5 [22400/60000 (37%)]\tLoss: 0.022470\n",
            "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 0.021808\n",
            "Train Epoch: 5 [23680/60000 (39%)]\tLoss: 0.040699\n",
            "Train Epoch: 5 [24320/60000 (41%)]\tLoss: 0.056012\n",
            "Train Epoch: 5 [24960/60000 (42%)]\tLoss: 0.029047\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.083291\n",
            "Train Epoch: 5 [26240/60000 (44%)]\tLoss: 0.008490\n",
            "Train Epoch: 5 [26880/60000 (45%)]\tLoss: 0.058916\n",
            "Train Epoch: 5 [27520/60000 (46%)]\tLoss: 0.136261\n",
            "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 0.059386\n",
            "Train Epoch: 5 [28800/60000 (48%)]\tLoss: 0.160867\n",
            "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 0.035637\n",
            "Train Epoch: 5 [30080/60000 (50%)]\tLoss: 0.080617\n",
            "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.038940\n",
            "Train Epoch: 5 [31360/60000 (52%)]\tLoss: 0.009816\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.028544\n",
            "Train Epoch: 5 [32640/60000 (54%)]\tLoss: 0.013513\n",
            "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 0.106375\n",
            "Train Epoch: 5 [33920/60000 (57%)]\tLoss: 0.094125\n",
            "Train Epoch: 5 [34560/60000 (58%)]\tLoss: 0.129159\n",
            "Train Epoch: 5 [35200/60000 (59%)]\tLoss: 0.038268\n",
            "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 0.076671\n",
            "Train Epoch: 5 [36480/60000 (61%)]\tLoss: 0.025641\n",
            "Train Epoch: 5 [37120/60000 (62%)]\tLoss: 0.034968\n",
            "Train Epoch: 5 [37760/60000 (63%)]\tLoss: 0.022538\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.052266\n",
            "Train Epoch: 5 [39040/60000 (65%)]\tLoss: 0.051189\n",
            "Train Epoch: 5 [39680/60000 (66%)]\tLoss: 0.014761\n",
            "Train Epoch: 5 [40320/60000 (67%)]\tLoss: 0.103996\n",
            "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.091085\n",
            "Train Epoch: 5 [41600/60000 (69%)]\tLoss: 0.059273\n",
            "Train Epoch: 5 [42240/60000 (70%)]\tLoss: 0.131553\n",
            "Train Epoch: 5 [42880/60000 (71%)]\tLoss: 0.086729\n",
            "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 0.047971\n",
            "Train Epoch: 5 [44160/60000 (74%)]\tLoss: 0.132430\n",
            "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.015233\n",
            "Train Epoch: 5 [45440/60000 (76%)]\tLoss: 0.062690\n",
            "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 0.061721\n",
            "Train Epoch: 5 [46720/60000 (78%)]\tLoss: 0.068324\n",
            "Train Epoch: 5 [47360/60000 (79%)]\tLoss: 0.062383\n",
            "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.117090\n",
            "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 0.086299\n",
            "Train Epoch: 5 [49280/60000 (82%)]\tLoss: 0.076749\n",
            "Train Epoch: 5 [49920/60000 (83%)]\tLoss: 0.089171\n",
            "Train Epoch: 5 [50560/60000 (84%)]\tLoss: 0.010177\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.100671\n",
            "Train Epoch: 5 [51840/60000 (86%)]\tLoss: 0.118301\n",
            "Train Epoch: 5 [52480/60000 (87%)]\tLoss: 0.174133\n",
            "Train Epoch: 5 [53120/60000 (88%)]\tLoss: 0.002953\n",
            "Train Epoch: 5 [53760/60000 (90%)]\tLoss: 0.038771\n",
            "Train Epoch: 5 [54400/60000 (91%)]\tLoss: 0.012935\n",
            "Train Epoch: 5 [55040/60000 (92%)]\tLoss: 0.012328\n",
            "Train Epoch: 5 [55680/60000 (93%)]\tLoss: 0.006518\n",
            "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 0.135945\n",
            "Train Epoch: 5 [56960/60000 (95%)]\tLoss: 0.118385\n",
            "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.052313\n",
            "Train Epoch: 5 [58240/60000 (97%)]\tLoss: 0.028378\n",
            "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 0.075084\n",
            "Train Epoch: 5 [59520/60000 (99%)]\tLoss: 0.030977\n",
            "\n",
            "Test set: Average loss: 0.0564, Accuracy: 9811/10000 (98%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.094989\n",
            "Train Epoch: 6 [640/60000 (1%)]\tLoss: 0.069586\n",
            "Train Epoch: 6 [1280/60000 (2%)]\tLoss: 0.073129\n",
            "Train Epoch: 6 [1920/60000 (3%)]\tLoss: 0.099416\n",
            "Train Epoch: 6 [2560/60000 (4%)]\tLoss: 0.075665\n",
            "Train Epoch: 6 [3200/60000 (5%)]\tLoss: 0.059107\n",
            "Train Epoch: 6 [3840/60000 (6%)]\tLoss: 0.143332\n",
            "Train Epoch: 6 [4480/60000 (7%)]\tLoss: 0.079283\n",
            "Train Epoch: 6 [5120/60000 (9%)]\tLoss: 0.177656\n",
            "Train Epoch: 6 [5760/60000 (10%)]\tLoss: 0.095598\n",
            "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.045126\n",
            "Train Epoch: 6 [7040/60000 (12%)]\tLoss: 0.041158\n",
            "Train Epoch: 6 [7680/60000 (13%)]\tLoss: 0.062188\n",
            "Train Epoch: 6 [8320/60000 (14%)]\tLoss: 0.036132\n",
            "Train Epoch: 6 [8960/60000 (15%)]\tLoss: 0.029942\n",
            "Train Epoch: 6 [9600/60000 (16%)]\tLoss: 0.089786\n",
            "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 0.005646\n",
            "Train Epoch: 6 [10880/60000 (18%)]\tLoss: 0.044119\n",
            "Train Epoch: 6 [11520/60000 (19%)]\tLoss: 0.029311\n",
            "Train Epoch: 6 [12160/60000 (20%)]\tLoss: 0.006738\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.020619\n",
            "Train Epoch: 6 [13440/60000 (22%)]\tLoss: 0.041411\n",
            "Train Epoch: 6 [14080/60000 (23%)]\tLoss: 0.012855\n",
            "Train Epoch: 6 [14720/60000 (25%)]\tLoss: 0.081992\n",
            "Train Epoch: 6 [15360/60000 (26%)]\tLoss: 0.017200\n",
            "Train Epoch: 6 [16000/60000 (27%)]\tLoss: 0.006121\n",
            "Train Epoch: 6 [16640/60000 (28%)]\tLoss: 0.107779\n",
            "Train Epoch: 6 [17280/60000 (29%)]\tLoss: 0.042432\n",
            "Train Epoch: 6 [17920/60000 (30%)]\tLoss: 0.095295\n",
            "Train Epoch: 6 [18560/60000 (31%)]\tLoss: 0.048014\n",
            "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.067940\n",
            "Train Epoch: 6 [19840/60000 (33%)]\tLoss: 0.261133\n",
            "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 0.114004\n",
            "Train Epoch: 6 [21120/60000 (35%)]\tLoss: 0.085395\n",
            "Train Epoch: 6 [21760/60000 (36%)]\tLoss: 0.021086\n",
            "Train Epoch: 6 [22400/60000 (37%)]\tLoss: 0.018580\n",
            "Train Epoch: 6 [23040/60000 (38%)]\tLoss: 0.012994\n",
            "Train Epoch: 6 [23680/60000 (39%)]\tLoss: 0.022353\n",
            "Train Epoch: 6 [24320/60000 (41%)]\tLoss: 0.034747\n",
            "Train Epoch: 6 [24960/60000 (42%)]\tLoss: 0.020541\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.087898\n",
            "Train Epoch: 6 [26240/60000 (44%)]\tLoss: 0.113053\n",
            "Train Epoch: 6 [26880/60000 (45%)]\tLoss: 0.013755\n",
            "Train Epoch: 6 [27520/60000 (46%)]\tLoss: 0.018615\n",
            "Train Epoch: 6 [28160/60000 (47%)]\tLoss: 0.076043\n",
            "Train Epoch: 6 [28800/60000 (48%)]\tLoss: 0.023634\n",
            "Train Epoch: 6 [29440/60000 (49%)]\tLoss: 0.012057\n",
            "Train Epoch: 6 [30080/60000 (50%)]\tLoss: 0.021800\n",
            "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 0.025143\n",
            "Train Epoch: 6 [31360/60000 (52%)]\tLoss: 0.003189\n",
            "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.085556\n",
            "Train Epoch: 6 [32640/60000 (54%)]\tLoss: 0.060709\n",
            "Train Epoch: 6 [33280/60000 (55%)]\tLoss: 0.013467\n",
            "Train Epoch: 6 [33920/60000 (57%)]\tLoss: 0.069217\n",
            "Train Epoch: 6 [34560/60000 (58%)]\tLoss: 0.056527\n",
            "Train Epoch: 6 [35200/60000 (59%)]\tLoss: 0.086670\n",
            "Train Epoch: 6 [35840/60000 (60%)]\tLoss: 0.049867\n",
            "Train Epoch: 6 [36480/60000 (61%)]\tLoss: 0.027539\n",
            "Train Epoch: 6 [37120/60000 (62%)]\tLoss: 0.019482\n",
            "Train Epoch: 6 [37760/60000 (63%)]\tLoss: 0.060501\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.091288\n",
            "Train Epoch: 6 [39040/60000 (65%)]\tLoss: 0.018111\n",
            "Train Epoch: 6 [39680/60000 (66%)]\tLoss: 0.013687\n",
            "Train Epoch: 6 [40320/60000 (67%)]\tLoss: 0.091729\n",
            "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 0.051312\n",
            "Train Epoch: 6 [41600/60000 (69%)]\tLoss: 0.066322\n",
            "Train Epoch: 6 [42240/60000 (70%)]\tLoss: 0.029323\n",
            "Train Epoch: 6 [42880/60000 (71%)]\tLoss: 0.082865\n",
            "Train Epoch: 6 [43520/60000 (72%)]\tLoss: 0.041667\n",
            "Train Epoch: 6 [44160/60000 (74%)]\tLoss: 0.073153\n",
            "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.046934\n",
            "Train Epoch: 6 [45440/60000 (76%)]\tLoss: 0.014677\n",
            "Train Epoch: 6 [46080/60000 (77%)]\tLoss: 0.023495\n",
            "Train Epoch: 6 [46720/60000 (78%)]\tLoss: 0.016082\n",
            "Train Epoch: 6 [47360/60000 (79%)]\tLoss: 0.083124\n",
            "Train Epoch: 6 [48000/60000 (80%)]\tLoss: 0.113803\n",
            "Train Epoch: 6 [48640/60000 (81%)]\tLoss: 0.062283\n",
            "Train Epoch: 6 [49280/60000 (82%)]\tLoss: 0.082326\n",
            "Train Epoch: 6 [49920/60000 (83%)]\tLoss: 0.050655\n",
            "Train Epoch: 6 [50560/60000 (84%)]\tLoss: 0.032821\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.006199\n",
            "Train Epoch: 6 [51840/60000 (86%)]\tLoss: 0.058221\n",
            "Train Epoch: 6 [52480/60000 (87%)]\tLoss: 0.027522\n",
            "Train Epoch: 6 [53120/60000 (88%)]\tLoss: 0.038371\n",
            "Train Epoch: 6 [53760/60000 (90%)]\tLoss: 0.016807\n",
            "Train Epoch: 6 [54400/60000 (91%)]\tLoss: 0.007978\n",
            "Train Epoch: 6 [55040/60000 (92%)]\tLoss: 0.061885\n",
            "Train Epoch: 6 [55680/60000 (93%)]\tLoss: 0.150667\n",
            "Train Epoch: 6 [56320/60000 (94%)]\tLoss: 0.066074\n",
            "Train Epoch: 6 [56960/60000 (95%)]\tLoss: 0.028431\n",
            "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.013909\n",
            "Train Epoch: 6 [58240/60000 (97%)]\tLoss: 0.078120\n",
            "Train Epoch: 6 [58880/60000 (98%)]\tLoss: 0.067762\n",
            "Train Epoch: 6 [59520/60000 (99%)]\tLoss: 0.084835\n",
            "\n",
            "Test set: Average loss: 0.0515, Accuracy: 9829/10000 (98%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.076543\n",
            "Train Epoch: 7 [640/60000 (1%)]\tLoss: 0.136803\n",
            "Train Epoch: 7 [1280/60000 (2%)]\tLoss: 0.010882\n",
            "Train Epoch: 7 [1920/60000 (3%)]\tLoss: 0.029291\n",
            "Train Epoch: 7 [2560/60000 (4%)]\tLoss: 0.018748\n",
            "Train Epoch: 7 [3200/60000 (5%)]\tLoss: 0.052610\n",
            "Train Epoch: 7 [3840/60000 (6%)]\tLoss: 0.011522\n",
            "Train Epoch: 7 [4480/60000 (7%)]\tLoss: 0.041250\n",
            "Train Epoch: 7 [5120/60000 (9%)]\tLoss: 0.016745\n",
            "Train Epoch: 7 [5760/60000 (10%)]\tLoss: 0.125773\n",
            "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.048657\n",
            "Train Epoch: 7 [7040/60000 (12%)]\tLoss: 0.062014\n",
            "Train Epoch: 7 [7680/60000 (13%)]\tLoss: 0.056435\n",
            "Train Epoch: 7 [8320/60000 (14%)]\tLoss: 0.095839\n",
            "Train Epoch: 7 [8960/60000 (15%)]\tLoss: 0.027920\n",
            "Train Epoch: 7 [9600/60000 (16%)]\tLoss: 0.085008\n",
            "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 0.007447\n",
            "Train Epoch: 7 [10880/60000 (18%)]\tLoss: 0.046253\n",
            "Train Epoch: 7 [11520/60000 (19%)]\tLoss: 0.056351\n",
            "Train Epoch: 7 [12160/60000 (20%)]\tLoss: 0.089086\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.012049\n",
            "Train Epoch: 7 [13440/60000 (22%)]\tLoss: 0.023006\n",
            "Train Epoch: 7 [14080/60000 (23%)]\tLoss: 0.134106\n",
            "Train Epoch: 7 [14720/60000 (25%)]\tLoss: 0.042692\n",
            "Train Epoch: 7 [15360/60000 (26%)]\tLoss: 0.008407\n",
            "Train Epoch: 7 [16000/60000 (27%)]\tLoss: 0.030303\n",
            "Train Epoch: 7 [16640/60000 (28%)]\tLoss: 0.066457\n",
            "Train Epoch: 7 [17280/60000 (29%)]\tLoss: 0.047058\n",
            "Train Epoch: 7 [17920/60000 (30%)]\tLoss: 0.034863\n",
            "Train Epoch: 7 [18560/60000 (31%)]\tLoss: 0.060408\n",
            "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.054849\n",
            "Train Epoch: 7 [19840/60000 (33%)]\tLoss: 0.017638\n",
            "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 0.084778\n",
            "Train Epoch: 7 [21120/60000 (35%)]\tLoss: 0.017938\n",
            "Train Epoch: 7 [21760/60000 (36%)]\tLoss: 0.031567\n",
            "Train Epoch: 7 [22400/60000 (37%)]\tLoss: 0.041047\n",
            "Train Epoch: 7 [23040/60000 (38%)]\tLoss: 0.065341\n",
            "Train Epoch: 7 [23680/60000 (39%)]\tLoss: 0.064469\n",
            "Train Epoch: 7 [24320/60000 (41%)]\tLoss: 0.045840\n",
            "Train Epoch: 7 [24960/60000 (42%)]\tLoss: 0.069921\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.061839\n",
            "Train Epoch: 7 [26240/60000 (44%)]\tLoss: 0.037416\n",
            "Train Epoch: 7 [26880/60000 (45%)]\tLoss: 0.040920\n",
            "Train Epoch: 7 [27520/60000 (46%)]\tLoss: 0.026754\n",
            "Train Epoch: 7 [28160/60000 (47%)]\tLoss: 0.024262\n",
            "Train Epoch: 7 [28800/60000 (48%)]\tLoss: 0.041434\n",
            "Train Epoch: 7 [29440/60000 (49%)]\tLoss: 0.042599\n",
            "Train Epoch: 7 [30080/60000 (50%)]\tLoss: 0.110273\n",
            "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 0.037209\n",
            "Train Epoch: 7 [31360/60000 (52%)]\tLoss: 0.043985\n",
            "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.079973\n",
            "Train Epoch: 7 [32640/60000 (54%)]\tLoss: 0.068012\n",
            "Train Epoch: 7 [33280/60000 (55%)]\tLoss: 0.018437\n",
            "Train Epoch: 7 [33920/60000 (57%)]\tLoss: 0.072705\n",
            "Train Epoch: 7 [34560/60000 (58%)]\tLoss: 0.041196\n",
            "Train Epoch: 7 [35200/60000 (59%)]\tLoss: 0.013123\n",
            "Train Epoch: 7 [35840/60000 (60%)]\tLoss: 0.196368\n",
            "Train Epoch: 7 [36480/60000 (61%)]\tLoss: 0.220852\n",
            "Train Epoch: 7 [37120/60000 (62%)]\tLoss: 0.070861\n",
            "Train Epoch: 7 [37760/60000 (63%)]\tLoss: 0.021196\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.014611\n",
            "Train Epoch: 7 [39040/60000 (65%)]\tLoss: 0.033896\n",
            "Train Epoch: 7 [39680/60000 (66%)]\tLoss: 0.008819\n",
            "Train Epoch: 7 [40320/60000 (67%)]\tLoss: 0.119300\n",
            "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 0.183503\n",
            "Train Epoch: 7 [41600/60000 (69%)]\tLoss: 0.189470\n",
            "Train Epoch: 7 [42240/60000 (70%)]\tLoss: 0.132442\n",
            "Train Epoch: 7 [42880/60000 (71%)]\tLoss: 0.004114\n",
            "Train Epoch: 7 [43520/60000 (72%)]\tLoss: 0.020123\n",
            "Train Epoch: 7 [44160/60000 (74%)]\tLoss: 0.016981\n",
            "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.020615\n",
            "Train Epoch: 7 [45440/60000 (76%)]\tLoss: 0.024496\n",
            "Train Epoch: 7 [46080/60000 (77%)]\tLoss: 0.039167\n",
            "Train Epoch: 7 [46720/60000 (78%)]\tLoss: 0.108497\n",
            "Train Epoch: 7 [47360/60000 (79%)]\tLoss: 0.044510\n",
            "Train Epoch: 7 [48000/60000 (80%)]\tLoss: 0.027324\n",
            "Train Epoch: 7 [48640/60000 (81%)]\tLoss: 0.048687\n",
            "Train Epoch: 7 [49280/60000 (82%)]\tLoss: 0.012317\n",
            "Train Epoch: 7 [49920/60000 (83%)]\tLoss: 0.013220\n",
            "Train Epoch: 7 [50560/60000 (84%)]\tLoss: 0.098860\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.069835\n",
            "Train Epoch: 7 [51840/60000 (86%)]\tLoss: 0.137911\n",
            "Train Epoch: 7 [52480/60000 (87%)]\tLoss: 0.045451\n",
            "Train Epoch: 7 [53120/60000 (88%)]\tLoss: 0.062809\n",
            "Train Epoch: 7 [53760/60000 (90%)]\tLoss: 0.013390\n",
            "Train Epoch: 7 [54400/60000 (91%)]\tLoss: 0.016064\n",
            "Train Epoch: 7 [55040/60000 (92%)]\tLoss: 0.106392\n",
            "Train Epoch: 7 [55680/60000 (93%)]\tLoss: 0.095170\n",
            "Train Epoch: 7 [56320/60000 (94%)]\tLoss: 0.009951\n",
            "Train Epoch: 7 [56960/60000 (95%)]\tLoss: 0.049826\n",
            "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.254669\n",
            "Train Epoch: 7 [58240/60000 (97%)]\tLoss: 0.041903\n",
            "Train Epoch: 7 [58880/60000 (98%)]\tLoss: 0.045897\n",
            "Train Epoch: 7 [59520/60000 (99%)]\tLoss: 0.025953\n",
            "\n",
            "Test set: Average loss: 0.0472, Accuracy: 9838/10000 (98%)\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.077745\n",
            "Train Epoch: 8 [640/60000 (1%)]\tLoss: 0.009606\n",
            "Train Epoch: 8 [1280/60000 (2%)]\tLoss: 0.062261\n",
            "Train Epoch: 8 [1920/60000 (3%)]\tLoss: 0.051987\n",
            "Train Epoch: 8 [2560/60000 (4%)]\tLoss: 0.090859\n",
            "Train Epoch: 8 [3200/60000 (5%)]\tLoss: 0.047680\n",
            "Train Epoch: 8 [3840/60000 (6%)]\tLoss: 0.008629\n",
            "Train Epoch: 8 [4480/60000 (7%)]\tLoss: 0.008867\n",
            "Train Epoch: 8 [5120/60000 (9%)]\tLoss: 0.217247\n",
            "Train Epoch: 8 [5760/60000 (10%)]\tLoss: 0.010778\n",
            "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.006702\n",
            "Train Epoch: 8 [7040/60000 (12%)]\tLoss: 0.004117\n",
            "Train Epoch: 8 [7680/60000 (13%)]\tLoss: 0.120889\n",
            "Train Epoch: 8 [8320/60000 (14%)]\tLoss: 0.047832\n",
            "Train Epoch: 8 [8960/60000 (15%)]\tLoss: 0.024547\n",
            "Train Epoch: 8 [9600/60000 (16%)]\tLoss: 0.025324\n",
            "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 0.025628\n",
            "Train Epoch: 8 [10880/60000 (18%)]\tLoss: 0.058266\n",
            "Train Epoch: 8 [11520/60000 (19%)]\tLoss: 0.007453\n",
            "Train Epoch: 8 [12160/60000 (20%)]\tLoss: 0.036226\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.009100\n",
            "Train Epoch: 8 [13440/60000 (22%)]\tLoss: 0.085753\n",
            "Train Epoch: 8 [14080/60000 (23%)]\tLoss: 0.074871\n",
            "Train Epoch: 8 [14720/60000 (25%)]\tLoss: 0.033376\n",
            "Train Epoch: 8 [15360/60000 (26%)]\tLoss: 0.130933\n",
            "Train Epoch: 8 [16000/60000 (27%)]\tLoss: 0.019390\n",
            "Train Epoch: 8 [16640/60000 (28%)]\tLoss: 0.064642\n",
            "Train Epoch: 8 [17280/60000 (29%)]\tLoss: 0.064219\n",
            "Train Epoch: 8 [17920/60000 (30%)]\tLoss: 0.033530\n",
            "Train Epoch: 8 [18560/60000 (31%)]\tLoss: 0.012819\n",
            "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.025178\n",
            "Train Epoch: 8 [19840/60000 (33%)]\tLoss: 0.064107\n",
            "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 0.113686\n",
            "Train Epoch: 8 [21120/60000 (35%)]\tLoss: 0.051607\n",
            "Train Epoch: 8 [21760/60000 (36%)]\tLoss: 0.048892\n",
            "Train Epoch: 8 [22400/60000 (37%)]\tLoss: 0.060542\n",
            "Train Epoch: 8 [23040/60000 (38%)]\tLoss: 0.014140\n",
            "Train Epoch: 8 [23680/60000 (39%)]\tLoss: 0.101321\n",
            "Train Epoch: 8 [24320/60000 (41%)]\tLoss: 0.054054\n",
            "Train Epoch: 8 [24960/60000 (42%)]\tLoss: 0.023448\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.057434\n",
            "Train Epoch: 8 [26240/60000 (44%)]\tLoss: 0.018741\n",
            "Train Epoch: 8 [26880/60000 (45%)]\tLoss: 0.009581\n",
            "Train Epoch: 8 [27520/60000 (46%)]\tLoss: 0.064534\n",
            "Train Epoch: 8 [28160/60000 (47%)]\tLoss: 0.030256\n",
            "Train Epoch: 8 [28800/60000 (48%)]\tLoss: 0.061073\n",
            "Train Epoch: 8 [29440/60000 (49%)]\tLoss: 0.159178\n",
            "Train Epoch: 8 [30080/60000 (50%)]\tLoss: 0.022253\n",
            "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 0.047841\n",
            "Train Epoch: 8 [31360/60000 (52%)]\tLoss: 0.032273\n",
            "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.033452\n",
            "Train Epoch: 8 [32640/60000 (54%)]\tLoss: 0.020842\n",
            "Train Epoch: 8 [33280/60000 (55%)]\tLoss: 0.039463\n",
            "Train Epoch: 8 [33920/60000 (57%)]\tLoss: 0.009157\n",
            "Train Epoch: 8 [34560/60000 (58%)]\tLoss: 0.075433\n",
            "Train Epoch: 8 [35200/60000 (59%)]\tLoss: 0.044764\n",
            "Train Epoch: 8 [35840/60000 (60%)]\tLoss: 0.020271\n",
            "Train Epoch: 8 [36480/60000 (61%)]\tLoss: 0.012896\n",
            "Train Epoch: 8 [37120/60000 (62%)]\tLoss: 0.092727\n",
            "Train Epoch: 8 [37760/60000 (63%)]\tLoss: 0.071947\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.009425\n",
            "Train Epoch: 8 [39040/60000 (65%)]\tLoss: 0.054843\n",
            "Train Epoch: 8 [39680/60000 (66%)]\tLoss: 0.162192\n",
            "Train Epoch: 8 [40320/60000 (67%)]\tLoss: 0.049514\n",
            "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 0.061448\n",
            "Train Epoch: 8 [41600/60000 (69%)]\tLoss: 0.075531\n",
            "Train Epoch: 8 [42240/60000 (70%)]\tLoss: 0.035235\n",
            "Train Epoch: 8 [42880/60000 (71%)]\tLoss: 0.007669\n",
            "Train Epoch: 8 [43520/60000 (72%)]\tLoss: 0.075869\n",
            "Train Epoch: 8 [44160/60000 (74%)]\tLoss: 0.009155\n",
            "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.076755\n",
            "Train Epoch: 8 [45440/60000 (76%)]\tLoss: 0.048966\n",
            "Train Epoch: 8 [46080/60000 (77%)]\tLoss: 0.005185\n",
            "Train Epoch: 8 [46720/60000 (78%)]\tLoss: 0.044346\n",
            "Train Epoch: 8 [47360/60000 (79%)]\tLoss: 0.185722\n",
            "Train Epoch: 8 [48000/60000 (80%)]\tLoss: 0.053782\n",
            "Train Epoch: 8 [48640/60000 (81%)]\tLoss: 0.043711\n",
            "Train Epoch: 8 [49280/60000 (82%)]\tLoss: 0.012875\n",
            "Train Epoch: 8 [49920/60000 (83%)]\tLoss: 0.088322\n",
            "Train Epoch: 8 [50560/60000 (84%)]\tLoss: 0.019705\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.063543\n",
            "Train Epoch: 8 [51840/60000 (86%)]\tLoss: 0.027956\n",
            "Train Epoch: 8 [52480/60000 (87%)]\tLoss: 0.048194\n",
            "Train Epoch: 8 [53120/60000 (88%)]\tLoss: 0.031972\n",
            "Train Epoch: 8 [53760/60000 (90%)]\tLoss: 0.028301\n",
            "Train Epoch: 8 [54400/60000 (91%)]\tLoss: 0.037250\n",
            "Train Epoch: 8 [55040/60000 (92%)]\tLoss: 0.026413\n",
            "Train Epoch: 8 [55680/60000 (93%)]\tLoss: 0.010488\n",
            "Train Epoch: 8 [56320/60000 (94%)]\tLoss: 0.032033\n",
            "Train Epoch: 8 [56960/60000 (95%)]\tLoss: 0.012620\n",
            "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.002668\n",
            "Train Epoch: 8 [58240/60000 (97%)]\tLoss: 0.049081\n",
            "Train Epoch: 8 [58880/60000 (98%)]\tLoss: 0.013680\n",
            "Train Epoch: 8 [59520/60000 (99%)]\tLoss: 0.036676\n",
            "\n",
            "Test set: Average loss: 0.0506, Accuracy: 9823/10000 (98%)\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.031742\n",
            "Train Epoch: 9 [640/60000 (1%)]\tLoss: 0.054205\n",
            "Train Epoch: 9 [1280/60000 (2%)]\tLoss: 0.068723\n",
            "Train Epoch: 9 [1920/60000 (3%)]\tLoss: 0.084012\n",
            "Train Epoch: 9 [2560/60000 (4%)]\tLoss: 0.034216\n",
            "Train Epoch: 9 [3200/60000 (5%)]\tLoss: 0.024852\n",
            "Train Epoch: 9 [3840/60000 (6%)]\tLoss: 0.017604\n",
            "Train Epoch: 9 [4480/60000 (7%)]\tLoss: 0.007774\n",
            "Train Epoch: 9 [5120/60000 (9%)]\tLoss: 0.031037\n",
            "Train Epoch: 9 [5760/60000 (10%)]\tLoss: 0.048848\n",
            "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.024796\n",
            "Train Epoch: 9 [7040/60000 (12%)]\tLoss: 0.058223\n",
            "Train Epoch: 9 [7680/60000 (13%)]\tLoss: 0.094083\n",
            "Train Epoch: 9 [8320/60000 (14%)]\tLoss: 0.038875\n",
            "Train Epoch: 9 [8960/60000 (15%)]\tLoss: 0.112449\n",
            "Train Epoch: 9 [9600/60000 (16%)]\tLoss: 0.093978\n",
            "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 0.068553\n",
            "Train Epoch: 9 [10880/60000 (18%)]\tLoss: 0.029712\n",
            "Train Epoch: 9 [11520/60000 (19%)]\tLoss: 0.005176\n",
            "Train Epoch: 9 [12160/60000 (20%)]\tLoss: 0.120091\n",
            "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.085617\n",
            "Train Epoch: 9 [13440/60000 (22%)]\tLoss: 0.066083\n",
            "Train Epoch: 9 [14080/60000 (23%)]\tLoss: 0.120430\n",
            "Train Epoch: 9 [14720/60000 (25%)]\tLoss: 0.024075\n",
            "Train Epoch: 9 [15360/60000 (26%)]\tLoss: 0.113253\n",
            "Train Epoch: 9 [16000/60000 (27%)]\tLoss: 0.066162\n",
            "Train Epoch: 9 [16640/60000 (28%)]\tLoss: 0.006898\n",
            "Train Epoch: 9 [17280/60000 (29%)]\tLoss: 0.013677\n",
            "Train Epoch: 9 [17920/60000 (30%)]\tLoss: 0.038901\n",
            "Train Epoch: 9 [18560/60000 (31%)]\tLoss: 0.060930\n",
            "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.166178\n",
            "Train Epoch: 9 [19840/60000 (33%)]\tLoss: 0.277812\n",
            "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 0.008611\n",
            "Train Epoch: 9 [21120/60000 (35%)]\tLoss: 0.077544\n",
            "Train Epoch: 9 [21760/60000 (36%)]\tLoss: 0.004418\n",
            "Train Epoch: 9 [22400/60000 (37%)]\tLoss: 0.088709\n",
            "Train Epoch: 9 [23040/60000 (38%)]\tLoss: 0.062407\n",
            "Train Epoch: 9 [23680/60000 (39%)]\tLoss: 0.004685\n",
            "Train Epoch: 9 [24320/60000 (41%)]\tLoss: 0.055987\n",
            "Train Epoch: 9 [24960/60000 (42%)]\tLoss: 0.070411\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.087197\n",
            "Train Epoch: 9 [26240/60000 (44%)]\tLoss: 0.056742\n",
            "Train Epoch: 9 [26880/60000 (45%)]\tLoss: 0.025859\n",
            "Train Epoch: 9 [27520/60000 (46%)]\tLoss: 0.052160\n",
            "Train Epoch: 9 [28160/60000 (47%)]\tLoss: 0.088508\n",
            "Train Epoch: 9 [28800/60000 (48%)]\tLoss: 0.006748\n",
            "Train Epoch: 9 [29440/60000 (49%)]\tLoss: 0.048203\n",
            "Train Epoch: 9 [30080/60000 (50%)]\tLoss: 0.127919\n",
            "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 0.005262\n",
            "Train Epoch: 9 [31360/60000 (52%)]\tLoss: 0.006909\n",
            "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.025339\n",
            "Train Epoch: 9 [32640/60000 (54%)]\tLoss: 0.057132\n",
            "Train Epoch: 9 [33280/60000 (55%)]\tLoss: 0.011547\n",
            "Train Epoch: 9 [33920/60000 (57%)]\tLoss: 0.023438\n",
            "Train Epoch: 9 [34560/60000 (58%)]\tLoss: 0.051190\n",
            "Train Epoch: 9 [35200/60000 (59%)]\tLoss: 0.018952\n",
            "Train Epoch: 9 [35840/60000 (60%)]\tLoss: 0.038856\n",
            "Train Epoch: 9 [36480/60000 (61%)]\tLoss: 0.029572\n",
            "Train Epoch: 9 [37120/60000 (62%)]\tLoss: 0.058624\n",
            "Train Epoch: 9 [37760/60000 (63%)]\tLoss: 0.022607\n",
            "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.145112\n",
            "Train Epoch: 9 [39040/60000 (65%)]\tLoss: 0.114892\n",
            "Train Epoch: 9 [39680/60000 (66%)]\tLoss: 0.028896\n",
            "Train Epoch: 9 [40320/60000 (67%)]\tLoss: 0.020745\n",
            "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 0.035523\n",
            "Train Epoch: 9 [41600/60000 (69%)]\tLoss: 0.016081\n",
            "Train Epoch: 9 [42240/60000 (70%)]\tLoss: 0.015857\n",
            "Train Epoch: 9 [42880/60000 (71%)]\tLoss: 0.037001\n",
            "Train Epoch: 9 [43520/60000 (72%)]\tLoss: 0.100808\n",
            "Train Epoch: 9 [44160/60000 (74%)]\tLoss: 0.034411\n",
            "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.027198\n",
            "Train Epoch: 9 [45440/60000 (76%)]\tLoss: 0.072631\n",
            "Train Epoch: 9 [46080/60000 (77%)]\tLoss: 0.005267\n",
            "Train Epoch: 9 [46720/60000 (78%)]\tLoss: 0.006893\n",
            "Train Epoch: 9 [47360/60000 (79%)]\tLoss: 0.066640\n",
            "Train Epoch: 9 [48000/60000 (80%)]\tLoss: 0.097611\n",
            "Train Epoch: 9 [48640/60000 (81%)]\tLoss: 0.013677\n",
            "Train Epoch: 9 [49280/60000 (82%)]\tLoss: 0.076180\n",
            "Train Epoch: 9 [49920/60000 (83%)]\tLoss: 0.014006\n",
            "Train Epoch: 9 [50560/60000 (84%)]\tLoss: 0.017925\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.052612\n",
            "Train Epoch: 9 [51840/60000 (86%)]\tLoss: 0.094074\n",
            "Train Epoch: 9 [52480/60000 (87%)]\tLoss: 0.107575\n",
            "Train Epoch: 9 [53120/60000 (88%)]\tLoss: 0.139901\n",
            "Train Epoch: 9 [53760/60000 (90%)]\tLoss: 0.073359\n",
            "Train Epoch: 9 [54400/60000 (91%)]\tLoss: 0.012459\n",
            "Train Epoch: 9 [55040/60000 (92%)]\tLoss: 0.011555\n",
            "Train Epoch: 9 [55680/60000 (93%)]\tLoss: 0.013691\n",
            "Train Epoch: 9 [56320/60000 (94%)]\tLoss: 0.004303\n",
            "Train Epoch: 9 [56960/60000 (95%)]\tLoss: 0.038888\n",
            "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.047723\n",
            "Train Epoch: 9 [58240/60000 (97%)]\tLoss: 0.015876\n",
            "Train Epoch: 9 [58880/60000 (98%)]\tLoss: 0.029448\n",
            "Train Epoch: 9 [59520/60000 (99%)]\tLoss: 0.045573\n",
            "\n",
            "Test set: Average loss: 0.0521, Accuracy: 9836/10000 (98%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FN2fvNHNC76b"
      },
      "id": "FN2fvNHNC76b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lecture 12"
      ],
      "metadata": {
        "id": "_DSXis_FDRW8"
      },
      "id": "_DSXis_FDRW8"
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.autograd import Variable\n"
      ],
      "metadata": {
        "id": "Sp3FZQC8DTPv"
      },
      "id": "Sp3FZQC8DTPv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx2char = ['h', 'i', 'e', 'l', 'o']\n",
        "# Teach hihell -> ihello\n",
        "x_data = [[0, 1, 0, 2, 3, 3]]   # hihell\n",
        "y_data = [1, 0, 2, 3, 3, 4]    # ihello"
      ],
      "metadata": {
        "id": "xy6V1g8WDXrs"
      },
      "id": "xy6V1g8WDXrs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# As we have one batch of samples, we will change them to variables only once\n",
        "inputs = Variable(torch.LongTensor(x_data))\n",
        "labels = Variable(torch.LongTensor(y_data))"
      ],
      "metadata": {
        "id": "dtVP0oUpDcE2"
      },
      "id": "dtVP0oUpDcE2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = 5\n",
        "input_size = 5\n",
        "embedding_size = 10  # embedding size\n",
        "hidden_size = 5  # output from the LSTM. 5 to directly predict one-hot\n",
        "batch_size = 1   # one sentence\n",
        "sequence_length = 6  # |ihello| == 6\n",
        "num_layers = 1  # one-layer rnn"
      ],
      "metadata": {
        "id": "aW8qJNqCDddJ"
      },
      "id": "aW8qJNqCDddJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "\n",
        "    def __init__(self, num_layers, hidden_size):\n",
        "        super(Model, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "        self.rnn = nn.RNN(input_size=embedding_size,\n",
        "                          hidden_size=5, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Initialize hidden and cell states\n",
        "        # (num_layers * num_directions, batch, hidden_size)\n",
        "        h_0 = Variable(torch.zeros(\n",
        "            self.num_layers, x.size(0), self.hidden_size))\n",
        "\n",
        "        emb = self.embedding(x)\n",
        "        emb = emb.view(batch_size, sequence_length, -1)\n",
        "\n",
        "        # Propagate embedding through RNN\n",
        "        # Input: (batch, seq_len, embedding_size)\n",
        "        # h_0: (num_layers * num_directions, batch, hidden_size)\n",
        "        out, _ = self.rnn(emb, h_0)\n",
        "        return self.fc(out.view(-1, num_classes))"
      ],
      "metadata": {
        "id": "3kGiuw3BDeyo"
      },
      "id": "3kGiuw3BDeyo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate RNN model\n",
        "model = Model(num_layers, hidden_size)\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9sGb-xf9Dgiz",
        "outputId": "d99caaac-73e0-401b-ab71-e517b0c3130f"
      },
      "id": "9sGb-xf9Dgiz",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model(\n",
            "  (embedding): Embedding(5, 10)\n",
            "  (rnn): RNN(10, 5, batch_first=True)\n",
            "  (fc): Linear(in_features=5, out_features=5, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set loss and optimizer function\n",
        "# CrossEntropyLoss = LogSoftmax + NLLLoss\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n"
      ],
      "metadata": {
        "id": "Qeh97Kn-DiCb"
      },
      "id": "Qeh97Kn-DiCb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "for epoch in range(100):\n",
        "    outputs = model(inputs)\n",
        "    optimizer.zero_grad()\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    _, idx = outputs.max(1)\n",
        "    idx = idx.data.numpy()\n",
        "    result_str = [idx2char[c] for c in idx.squeeze()]\n",
        "    print(\"epoch: %d, loss: %1.3f\" % (epoch + 1, loss.item()))\n",
        "    print(\"Predicted string: \", ''.join(result_str))\n",
        "\n",
        "print(\"Learning finished!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ms6ME-ABDjsx",
        "outputId": "98408ecd-ba45-4d3d-9a01-bcb860b87b27"
      },
      "id": "Ms6ME-ABDjsx",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 1, loss: 1.617\n",
            "Predicted string:  iiilee\n",
            "epoch: 2, loss: 1.347\n",
            "Predicted string:  iiillo\n",
            "epoch: 3, loss: 1.172\n",
            "Predicted string:  iiillo\n",
            "epoch: 4, loss: 0.994\n",
            "Predicted string:  iiillo\n",
            "epoch: 5, loss: 0.815\n",
            "Predicted string:  iiillo\n",
            "epoch: 6, loss: 0.685\n",
            "Predicted string:  ihillo\n",
            "epoch: 7, loss: 0.580\n",
            "Predicted string:  ihillo\n",
            "epoch: 8, loss: 0.504\n",
            "Predicted string:  ehello\n",
            "epoch: 9, loss: 0.452\n",
            "Predicted string:  ehello\n",
            "epoch: 10, loss: 0.405\n",
            "Predicted string:  ehello\n",
            "epoch: 11, loss: 0.354\n",
            "Predicted string:  ehello\n",
            "epoch: 12, loss: 0.294\n",
            "Predicted string:  ihello\n",
            "epoch: 13, loss: 0.238\n",
            "Predicted string:  ihello\n",
            "epoch: 14, loss: 0.196\n",
            "Predicted string:  ihello\n",
            "epoch: 15, loss: 0.164\n",
            "Predicted string:  ihello\n",
            "epoch: 16, loss: 0.138\n",
            "Predicted string:  ihello\n",
            "epoch: 17, loss: 0.116\n",
            "Predicted string:  ihello\n",
            "epoch: 18, loss: 0.098\n",
            "Predicted string:  ihello\n",
            "epoch: 19, loss: 0.082\n",
            "Predicted string:  ihello\n",
            "epoch: 20, loss: 0.068\n",
            "Predicted string:  ihello\n",
            "epoch: 21, loss: 0.055\n",
            "Predicted string:  ihello\n",
            "epoch: 22, loss: 0.045\n",
            "Predicted string:  ihello\n",
            "epoch: 23, loss: 0.037\n",
            "Predicted string:  ihello\n",
            "epoch: 24, loss: 0.031\n",
            "Predicted string:  ihello\n",
            "epoch: 25, loss: 0.027\n",
            "Predicted string:  ihello\n",
            "epoch: 26, loss: 0.024\n",
            "Predicted string:  ihello\n",
            "epoch: 27, loss: 0.021\n",
            "Predicted string:  ihello\n",
            "epoch: 28, loss: 0.019\n",
            "Predicted string:  ihello\n",
            "epoch: 29, loss: 0.017\n",
            "Predicted string:  ihello\n",
            "epoch: 30, loss: 0.015\n",
            "Predicted string:  ihello\n",
            "epoch: 31, loss: 0.014\n",
            "Predicted string:  ihello\n",
            "epoch: 32, loss: 0.013\n",
            "Predicted string:  ihello\n",
            "epoch: 33, loss: 0.012\n",
            "Predicted string:  ihello\n",
            "epoch: 34, loss: 0.011\n",
            "Predicted string:  ihello\n",
            "epoch: 35, loss: 0.010\n",
            "Predicted string:  ihello\n",
            "epoch: 36, loss: 0.009\n",
            "Predicted string:  ihello\n",
            "epoch: 37, loss: 0.009\n",
            "Predicted string:  ihello\n",
            "epoch: 38, loss: 0.008\n",
            "Predicted string:  ihello\n",
            "epoch: 39, loss: 0.008\n",
            "Predicted string:  ihello\n",
            "epoch: 40, loss: 0.007\n",
            "Predicted string:  ihello\n",
            "epoch: 41, loss: 0.007\n",
            "Predicted string:  ihello\n",
            "epoch: 42, loss: 0.006\n",
            "Predicted string:  ihello\n",
            "epoch: 43, loss: 0.006\n",
            "Predicted string:  ihello\n",
            "epoch: 44, loss: 0.006\n",
            "Predicted string:  ihello\n",
            "epoch: 45, loss: 0.006\n",
            "Predicted string:  ihello\n",
            "epoch: 46, loss: 0.005\n",
            "Predicted string:  ihello\n",
            "epoch: 47, loss: 0.005\n",
            "Predicted string:  ihello\n",
            "epoch: 48, loss: 0.005\n",
            "Predicted string:  ihello\n",
            "epoch: 49, loss: 0.005\n",
            "Predicted string:  ihello\n",
            "epoch: 50, loss: 0.005\n",
            "Predicted string:  ihello\n",
            "epoch: 51, loss: 0.005\n",
            "Predicted string:  ihello\n",
            "epoch: 52, loss: 0.005\n",
            "Predicted string:  ihello\n",
            "epoch: 53, loss: 0.004\n",
            "Predicted string:  ihello\n",
            "epoch: 54, loss: 0.004\n",
            "Predicted string:  ihello\n",
            "epoch: 55, loss: 0.004\n",
            "Predicted string:  ihello\n",
            "epoch: 56, loss: 0.004\n",
            "Predicted string:  ihello\n",
            "epoch: 57, loss: 0.004\n",
            "Predicted string:  ihello\n",
            "epoch: 58, loss: 0.004\n",
            "Predicted string:  ihello\n",
            "epoch: 59, loss: 0.004\n",
            "Predicted string:  ihello\n",
            "epoch: 60, loss: 0.004\n",
            "Predicted string:  ihello\n",
            "epoch: 61, loss: 0.004\n",
            "Predicted string:  ihello\n",
            "epoch: 62, loss: 0.004\n",
            "Predicted string:  ihello\n",
            "epoch: 63, loss: 0.004\n",
            "Predicted string:  ihello\n",
            "epoch: 64, loss: 0.004\n",
            "Predicted string:  ihello\n",
            "epoch: 65, loss: 0.004\n",
            "Predicted string:  ihello\n",
            "epoch: 66, loss: 0.003\n",
            "Predicted string:  ihello\n",
            "epoch: 67, loss: 0.003\n",
            "Predicted string:  ihello\n",
            "epoch: 68, loss: 0.003\n",
            "Predicted string:  ihello\n",
            "epoch: 69, loss: 0.003\n",
            "Predicted string:  ihello\n",
            "epoch: 70, loss: 0.003\n",
            "Predicted string:  ihello\n",
            "epoch: 71, loss: 0.003\n",
            "Predicted string:  ihello\n",
            "epoch: 72, loss: 0.003\n",
            "Predicted string:  ihello\n",
            "epoch: 73, loss: 0.003\n",
            "Predicted string:  ihello\n",
            "epoch: 74, loss: 0.003\n",
            "Predicted string:  ihello\n",
            "epoch: 75, loss: 0.003\n",
            "Predicted string:  ihello\n",
            "epoch: 76, loss: 0.003\n",
            "Predicted string:  ihello\n",
            "epoch: 77, loss: 0.003\n",
            "Predicted string:  ihello\n",
            "epoch: 78, loss: 0.003\n",
            "Predicted string:  ihello\n",
            "epoch: 79, loss: 0.003\n",
            "Predicted string:  ihello\n",
            "epoch: 80, loss: 0.003\n",
            "Predicted string:  ihello\n",
            "epoch: 81, loss: 0.003\n",
            "Predicted string:  ihello\n",
            "epoch: 82, loss: 0.003\n",
            "Predicted string:  ihello\n",
            "epoch: 83, loss: 0.003\n",
            "Predicted string:  ihello\n",
            "epoch: 84, loss: 0.003\n",
            "Predicted string:  ihello\n",
            "epoch: 85, loss: 0.003\n",
            "Predicted string:  ihello\n",
            "epoch: 86, loss: 0.003\n",
            "Predicted string:  ihello\n",
            "epoch: 87, loss: 0.003\n",
            "Predicted string:  ihello\n",
            "epoch: 88, loss: 0.003\n",
            "Predicted string:  ihello\n",
            "epoch: 89, loss: 0.003\n",
            "Predicted string:  ihello\n",
            "epoch: 90, loss: 0.003\n",
            "Predicted string:  ihello\n",
            "epoch: 91, loss: 0.003\n",
            "Predicted string:  ihello\n",
            "epoch: 92, loss: 0.003\n",
            "Predicted string:  ihello\n",
            "epoch: 93, loss: 0.003\n",
            "Predicted string:  ihello\n",
            "epoch: 94, loss: 0.003\n",
            "Predicted string:  ihello\n",
            "epoch: 95, loss: 0.003\n",
            "Predicted string:  ihello\n",
            "epoch: 96, loss: 0.003\n",
            "Predicted string:  ihello\n",
            "epoch: 97, loss: 0.003\n",
            "Predicted string:  ihello\n",
            "epoch: 98, loss: 0.003\n",
            "Predicted string:  ihello\n",
            "epoch: 99, loss: 0.002\n",
            "Predicted string:  ihello\n",
            "epoch: 100, loss: 0.002\n",
            "Predicted string:  ihello\n",
            "Learning finished!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lecture 13"
      ],
      "metadata": {
        "id": "miUAyduTD52n"
      },
      "id": "miUAyduTD52n"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import itertools"
      ],
      "metadata": {
        "id": "ik9mJmvtDlpB"
      },
      "id": "ik9mJmvtDlpB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def flatten(l):\n",
        "    return list(itertools.chain.from_iterable(l))\n",
        "\n",
        "seqs = ['ghatmasala', 'nicela', 'chutpakodas']"
      ],
      "metadata": {
        "id": "Q6P9E--CD-hE"
      },
      "id": "Q6P9E--CD-hE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make <pad> idx 0\n",
        "vocab = ['<pad>'] + sorted(list(set(flatten(seqs))))"
      ],
      "metadata": {
        "id": "eHARmfwZEAiE"
      },
      "id": "eHARmfwZEAiE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make model\n",
        "embedding_size = 3\n",
        "embed = nn.Embedding(len(vocab), embedding_size)\n",
        "lstm = nn.LSTM(embedding_size, 5)\n"
      ],
      "metadata": {
        "id": "oqolfqmDEB-q"
      },
      "id": "oqolfqmDEB-q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorized_seqs = [[vocab.index(tok) for tok in seq]for seq in seqs]\n",
        "print(\"vectorized_seqs\", vectorized_seqs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3VRoJYacEDdU",
        "outputId": "9b494659-3eca-4115-ff04-abd8a678f6e1"
      },
      "id": "3VRoJYacEDdU",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vectorized_seqs [[5, 6, 1, 15, 10, 1, 14, 1, 9, 1], [11, 7, 2, 4, 9, 1], [2, 6, 16, 15, 13, 1, 8, 12, 3, 1, 14]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print([x for x in map(len, vectorized_seqs)])\n",
        "# get the length of each seq in your batch\n",
        "seq_lengths = torch.LongTensor([x for x in map(len, vectorized_seqs)])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GN9ROtTDEFMN",
        "outputId": "025a0cdb-5074-429e-8c84-5540df0c8974"
      },
      "id": "GN9ROtTDEFMN",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[10, 6, 11]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dump padding everywhere, and place seqs on the left.\n",
        "# NOTE: you only need a tensor as big as your longest sequence\n",
        "seq_tensor = Variable(torch.zeros(\n",
        "    (len(vectorized_seqs), seq_lengths.max()))).long()\n",
        "for idx, (seq, seqlen) in enumerate(zip(vectorized_seqs, seq_lengths)):\n",
        "    seq_tensor[idx, :seqlen] = torch.LongTensor(seq)"
      ],
      "metadata": {
        "id": "3AXLtFK9EGlj"
      },
      "id": "3AXLtFK9EGlj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"seq_tensor\", seq_tensor)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FyqLV6gYEIU6",
        "outputId": "08cd41c1-942a-4a82-d53f-903dc66eb338"
      },
      "id": "FyqLV6gYEIU6",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "seq_tensor tensor([[ 5,  6,  1, 15, 10,  1, 14,  1,  9,  1,  0],\n",
            "        [11,  7,  2,  4,  9,  1,  0,  0,  0,  0,  0],\n",
            "        [ 2,  6, 16, 15, 13,  1,  8, 12,  3,  1, 14]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SORT YOUR TENSORS BY LENGTH!\n",
        "seq_lengths, perm_idx = seq_lengths.sort(0, descending=True)\n",
        "seq_tensor = seq_tensor[perm_idx]"
      ],
      "metadata": {
        "id": "_7BJGdWeEJeX"
      },
      "id": "_7BJGdWeEJeX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"seq_tensor after sorting\", seq_tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09v-JMhJEK8_",
        "outputId": "d0a14bce-06c3-4bf7-9b08-c5524f2fd8a8"
      },
      "id": "09v-JMhJEK8_",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "seq_tensor after sorting tensor([[ 2,  6, 16, 15, 13,  1,  8, 12,  3,  1, 14],\n",
            "        [ 5,  6,  1, 15, 10,  1, 14,  1,  9,  1,  0],\n",
            "        [11,  7,  2,  4,  9,  1,  0,  0,  0,  0,  0]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# utils.rnn lets you give (B,L,D) tensors where B is the batch size, L is the maxlength, if you use batch_first=True\n",
        "# Otherwise, give (L,B,D) tensors\n",
        "seq_tensor = seq_tensor.transpose(0, 1)  # (B,L,D) -> (L,B,D)\n",
        "print(\"seq_tensor after transposing\", seq_tensor.size(), seq_tensor.data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sr8XYe7iEOss",
        "outputId": "f591d5ec-0b22-438d-daec-a888de179f2b"
      },
      "id": "Sr8XYe7iEOss",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "seq_tensor after transposing torch.Size([11, 3]) tensor([[ 2,  5, 11],\n",
            "        [ 6,  6,  7],\n",
            "        [16,  1,  2],\n",
            "        [15, 15,  4],\n",
            "        [13, 10,  9],\n",
            "        [ 1,  1,  1],\n",
            "        [ 8, 14,  0],\n",
            "        [12,  1,  0],\n",
            "        [ 3,  9,  0],\n",
            "        [ 1,  1,  0],\n",
            "        [14,  0,  0]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# embed your sequences\n",
        "embeded_seq_tensor = embed(seq_tensor)\n",
        "print(\"seq_tensor after embeding\", embeded_seq_tensor.size(), seq_tensor.data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbQ8msi2EPft",
        "outputId": "d7744234-637f-4f9c-e0c2-28d72f43ad63"
      },
      "id": "jbQ8msi2EPft",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "seq_tensor after embeding torch.Size([11, 3, 3]) tensor([[ 2,  5, 11],\n",
            "        [ 6,  6,  7],\n",
            "        [16,  1,  2],\n",
            "        [15, 15,  4],\n",
            "        [13, 10,  9],\n",
            "        [ 1,  1,  1],\n",
            "        [ 8, 14,  0],\n",
            "        [12,  1,  0],\n",
            "        [ 3,  9,  0],\n",
            "        [ 1,  1,  0],\n",
            "        [14,  0,  0]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pack them up nicely\n",
        "packed_input = pack_padded_sequence(\n",
        "    embeded_seq_tensor, seq_lengths.cpu().numpy())\n"
      ],
      "metadata": {
        "id": "F8UXBmKGEQ-k"
      },
      "id": "F8UXBmKGEQ-k",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# throw them through your LSTM (remember to give batch_first=True here if\n",
        "# you packed with it)\n",
        "packed_output, (ht, ct) = lstm(packed_input)"
      ],
      "metadata": {
        "id": "3zR-qwQxESKT"
      },
      "id": "3zR-qwQxESKT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# unpack your output if required\n",
        "output, _ = pad_packed_sequence(packed_output)\n",
        "print(\"Lstm output\", output.size(), output.data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7KSQfUTIETWn",
        "outputId": "ced7d27f-bf7b-4ac2-c576-7ff89e1be9e7"
      },
      "id": "7KSQfUTIETWn",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lstm output torch.Size([11, 3, 5]) tensor([[[ 0.0492,  0.1417, -0.0195,  0.0463, -0.0839],\n",
            "         [ 0.0586,  0.0700,  0.0638,  0.1483, -0.0583],\n",
            "         [ 0.1596,  0.0654,  0.0292,  0.1216, -0.1123]],\n",
            "\n",
            "        [[ 0.1466,  0.0954,  0.0350,  0.1303, -0.0800],\n",
            "         [ 0.1542,  0.0706,  0.0710,  0.1683, -0.0744],\n",
            "         [ 0.2380,  0.1556,  0.0255,  0.0749, -0.1254]],\n",
            "\n",
            "        [[ 0.0832,  0.2071,  0.0676,  0.1880, -0.1439],\n",
            "         [-0.0155,  0.2665, -0.0456,  0.0357, -0.1395],\n",
            "         [ 0.1511,  0.2379,  0.0138,  0.1071, -0.1432]],\n",
            "\n",
            "        [[ 0.0750,  0.2104,  0.1076,  0.2351, -0.1295],\n",
            "         [ 0.0409,  0.2132,  0.0201,  0.1711, -0.1284],\n",
            "         [-0.0120,  0.3416,  0.0774,  0.2727, -0.1972]],\n",
            "\n",
            "        [[ 0.2076,  0.1614,  0.0456,  0.1254, -0.0987],\n",
            "         [-0.0166,  0.2833, -0.1430,  0.0058, -0.1297],\n",
            "         [-0.0225,  0.1272,  0.3439,  0.4157, -0.0581]],\n",
            "\n",
            "        [[-0.0007,  0.3286, -0.0964,  0.0279, -0.1555],\n",
            "         [-0.0909,  0.3449, -0.2922, -0.0231, -0.1563],\n",
            "         [-0.1385,  0.2588,  0.1672,  0.0938, -0.1219]],\n",
            "\n",
            "        [[ 0.0882,  0.1987,  0.0113,  0.1615, -0.1197],\n",
            "         [ 0.2214,  0.1659, -0.0871, -0.0344, -0.1202],\n",
            "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
            "\n",
            "        [[ 0.0364,  0.1420,  0.1087,  0.2252, -0.0750],\n",
            "         [ 0.0021,  0.3539, -0.3221, -0.0232, -0.1488],\n",
            "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
            "\n",
            "        [[-0.0218,  0.2105,  0.0316,  0.1375, -0.0852],\n",
            "         [-0.0122,  0.1016,  0.0177,  0.3219, -0.0256],\n",
            "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
            "\n",
            "        [[-0.1080,  0.2990, -0.1527,  0.0091, -0.1413],\n",
            "         [-0.1022,  0.2520, -0.1435,  0.0514, -0.0999],\n",
            "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
            "\n",
            "        [[ 0.2094,  0.1509, -0.0584, -0.0252, -0.1172],\n",
            "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Or if you just want the final hidden state?\n",
        "print(\"Last output\", ht[-1].size(), ht[-1].data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I1JJliKPEUZ7",
        "outputId": "b348194e-7ec1-4faf-b529-49f254524b3b"
      },
      "id": "I1JJliKPEUZ7",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last output torch.Size([3, 5]) tensor([[ 0.2094,  0.1509, -0.0584, -0.0252, -0.1172],\n",
            "        [-0.1022,  0.2520, -0.1435,  0.0514, -0.0999],\n",
            "        [-0.1385,  0.2588,  0.1672,  0.0938, -0.1219]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9Vcog1htEV85"
      },
      "id": "9Vcog1htEV85",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}